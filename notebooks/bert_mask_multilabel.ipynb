{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "sys.path.append('../code')\n",
    "from dataset import get_data, MaskedDataset, make_vocab, read_json\n",
    "\n",
    "from transformers import (\n",
    "    AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from models import MaskedRDModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_size = 5\n",
    "# model = MaskedRDModel.from_pretrained('bert-base-uncased')\n",
    "# model.initialize(mask_size=mask_size, multilabel=True)\n",
    "model = torch.load('../trained_models/MaskedRDModel_Epoch_1_at_2021-05-03_07:52:35.719441')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training data: 675715 word-def pairs\n",
      "Dev data: 75873 word-def pairs\n",
      "Test data: 1200 word-def pairs\n"
     ]
    }
   ],
   "source": [
    "d = get_data('../wantwords-english-baseline/data', word2vec=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_data_def, dev_data, test_data_seen, \\\n",
    "    test_data_unseen, test_data_desc = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_matrix, target2idx, idx2target = make_vocab(d, tokenizer, mask_size=mask_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16187, tensor([2338,  103,  103,  103,  103]), 'book')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target2idx maps target words to indices\n",
    "# target_matrix maps target indices to bpe sequences, padded/truncated to mask_size\n",
    "target2idx['book'], target_matrix[target2idx['book']], idx2target[target2idx['book']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_data = read_json('../data/wn_data.json')\n",
    "wn_categories = ['synonyms', 'hyponyms', 'hypernyms', 'related_forms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MaskedDataset(train_data + train_data_def, tokenizer, target2idx, wn_data=wn_data, wn_categories=wn_categories, mask_size=mask_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = MaskedDataset(dev_data, tokenizer, target2idx, \n",
    "                            wn_data=wn_data, wn_categories=wn_categories, mask_size=mask_size)\n",
    "test_dataset_seen = MaskedDataset(test_data_seen, tokenizer, target2idx, \n",
    "                                  wn_data=wn_data, wn_categories=wn_categories, mask_size=mask_size)\n",
    "test_dataset_unseen = MaskedDataset(test_data_unseen, tokenizer, target2idx, \n",
    "                                    wn_data=wn_data, wn_categories=wn_categories, mask_size=mask_size)\n",
    "test_dataset_desc = MaskedDataset(test_data_desc, tokenizer, target2idx, \n",
    "                                  wn_data=wn_data, wn_categories=wn_categories, mask_size=mask_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['classic',\n",
       "  'authorized',\n",
       "  'importance',\n",
       "  'authoritative',\n",
       "  'classical',\n",
       "  'definitive',\n",
       "  'important'],\n",
       " 'authoritative')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 1593\n",
    "\n",
    "[idx2target[x] for x in dev_dataset[index][-1].coalesce().indices().squeeze(0)], idx2target[dev_dataset[index][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_workers = 0\n",
    "\n",
    "loader_params = {\n",
    "    'pin_memory': False,\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': num_workers,\n",
    "    'collate_fn': train_dataset.collate_fn\n",
    "}\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, **{'shuffle': True, **loader_params})\n",
    "dev_loader = data.DataLoader(dev_dataset, **{'shuffle': True, **loader_params})\n",
    "test_loader_seen = data.DataLoader(test_dataset_seen, **{'shuffle': False, **loader_params})\n",
    "test_loader_unseen = data.DataLoader(test_dataset_unseen, **{'shuffle': False, **loader_params})\n",
    "test_loader_desc = data.DataLoader(test_dataset_desc, **{'shuffle': False, **loader_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from epoch 2\n",
    "epochs = 9\n",
    "lr = 1e-5 * 0.905\n",
    "optim = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=1, \n",
    "                                            num_training_steps=len(train_loader) * epochs)\n",
    "epoch = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "\n",
    "# lr = 1e-5\n",
    "# optim = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# warmup_duration = 0.05 # portion of the first epoch spent on lr warmup\n",
    "# scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=len(train_loader) * warmup_duration, \n",
    "#                                             num_training_steps=len(train_loader) * epochs)\n",
    "\n",
    "# epoch = 0\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:164mn9bf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9544<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ubuntu/dl/11785-Reverse-Dictionary-Project/notebooks/wandb/run-20210503_161909-164mn9bf/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ubuntu/dl/11785-Reverse-Dictionary-Project/notebooks/wandb/run-20210503_161909-164mn9bf/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">classic-firefly-41</strong>: <a href=\"https://wandb.ai/reverse-dict/reverse-dictionary/runs/164mn9bf\" target=\"_blank\">https://wandb.ai/reverse-dict/reverse-dictionary/runs/164mn9bf</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:164mn9bf). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">smooth-waterfall-42</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/reverse-dict/reverse-dictionary\" target=\"_blank\">https://wandb.ai/reverse-dict/reverse-dictionary</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/reverse-dict/reverse-dictionary/runs/1z5mbb8j\" target=\"_blank\">https://wandb.ai/reverse-dict/reverse-dictionary/runs/1z5mbb8j</a><br/>\n",
       "                Run data is saved locally in <code>/home/ubuntu/dl/11785-Reverse-Dictionary-Project/notebooks/wandb/run-20210503_161950-1z5mbb8j</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project='reverse-dictionary', entity='reverse-dict')\n",
    "\n",
    "config = wandb.config\n",
    "config.learning_rate = lr\n",
    "config.epochs = epochs\n",
    "config.batch_size = batch_size\n",
    "config.optimizer = type(optim).__name__\n",
    "config.scheduler = type(scheduler).__name__\n",
    "# config.warmup_duration = warmup_duration\n",
    "\n",
    "# wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matrix = target_matrix.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred, gt, test=False):\n",
    "    acc1 = acc10 = acc100 = 0\n",
    "    n = len(pred)\n",
    "    pred_rank = []\n",
    "    for p, word in zip(pred, gt):\n",
    "        if test:\n",
    "            loc = (p == word).nonzero(as_tuple=True)\n",
    "            if len(loc) != 0:\n",
    "                pred_rank.append(min(loc[-1], 1000))\n",
    "            else:\n",
    "                pred_rank.append(1000)\n",
    "        if word in p[:100]:\n",
    "            acc100 += 1\n",
    "            if word in p[:10]:\n",
    "                acc10 += 1\n",
    "                if word == p[0]:\n",
    "                    acc1 += 1\n",
    "    if test:\n",
    "        pred_rank = torch.tensor(pred_rank, dtype=torch.float32)\n",
    "        return (acc1, acc10, acc100, pred_rank)\n",
    "    else:\n",
    "        return acc1/n, acc10/n, acc100/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, name, log=False):\n",
    "    inc = 3\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc1 = test_acc10 = test_acc100 = test_rank_median = test_rank_variance = 0.0\n",
    "    total_seen = 0\n",
    "    all_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(loader)) as pbar:\n",
    "            for i, (x,y, wn_ids) in enumerate(loader):\n",
    "                if i % inc == 0 and i != 0:\n",
    "                    display_loss = test_loss / i\n",
    "                    pbar.set_description(f'Test Loss: {display_loss}')\n",
    "\n",
    "                x = x.to(device)\n",
    "                attention_mask = (x != train_dataset.pad_id)\n",
    "                y = y.to(device)\n",
    "                wn_ids = wn_ids.to_dense().to(device).float()\n",
    "\n",
    "#                 with autocast():\n",
    "                loss, out = model(input_ids=x, attention_mask=attention_mask, \n",
    "                                  target_matrix=target_matrix, ground_truth=y,\n",
    "                                  wn_ids=wn_ids, weight_gt=5)\n",
    "\n",
    "                test_loss += loss.detach()\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "                result, indices = torch.sort(out, descending=True)\n",
    "                \n",
    "                b = len(x)\n",
    "                acc1, acc10, acc100, pred_rank = evaluate(indices, y, test=True)\n",
    "                test_acc1 += acc1\n",
    "                test_acc10 += acc10\n",
    "                test_acc100 += acc100\n",
    "                total_seen += b\n",
    "                all_pred.extend(pred_rank)\n",
    "                \n",
    "                del x, y, out, loss\n",
    "                if i % 20 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "    test_loss /= len(loader)\n",
    "    test_acc1 /= total_seen\n",
    "    test_acc10 /= total_seen\n",
    "    test_acc100 /= total_seen\n",
    "    all_pred = torch.tensor(all_pred)\n",
    "    median = torch.median(all_pred)\n",
    "    var = torch.var(all_pred)**0.5\n",
    "    \n",
    "    print(f'{name}_test_loss:', test_loss)\n",
    "    print(f'{name}_test_acc1:', test_acc1)\n",
    "    print(f'{name}_test_acc10:', test_acc10)\n",
    "    print(f'{name}_test_acc100:', test_acc100)\n",
    "    print(f'{name}_test_rank_median:', median)\n",
    "    print(f'{name}_test_rank_variance', var)\n",
    "    \n",
    "    return {\n",
    "            f'{name}_test_loss': test_loss,\n",
    "            f'{name}_test_acc1': test_acc1,\n",
    "            f'{name}_test_acc10': test_acc10,\n",
    "            f'{name}_test_acc100': test_acc100,\n",
    "            f'{name}_test_rank_median': median,\n",
    "            f'{name}_test_rank_variance': var\n",
    "           }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16893 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training beginning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 144.08778381347656:   6%|▌         | 1035/16893 [06:28<1:29:22,  2.96it/s]"
     ]
    }
   ],
   "source": [
    "inc = 10\n",
    "losses = []\n",
    "print('Training beginning!')\n",
    "\n",
    "for epoch in range(epoch, epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    # Train on subset of training data to save time\n",
    "    with tqdm(total=len(train_loader)) as pbar:\n",
    "        for i, (x, y, wn_ids) in enumerate(train_loader):\n",
    "            if i % inc == 0 and i != 0:\n",
    "                display_loss = train_loss / i\n",
    "                pbar.set_description(f'Epoch {epoch+1}, Train Loss: {train_loss / i}')\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            x = x.to(device)\n",
    "            attention_mask = (x != train_dataset.pad_id)\n",
    "            y = y.to(device)\n",
    "            wn_ids = wn_ids.to_dense().to(device).float()\n",
    "            \n",
    "            loss, out = model(input_ids=x, attention_mask=attention_mask, \n",
    "                              target_matrix=target_matrix, ground_truth=y, \n",
    "                              wn_ids=wn_ids, weight_gt=5)\n",
    "            \n",
    "#             scaler.scale(loss).backward()\n",
    "            loss.backward()\n",
    "            \n",
    "#             scaler.unscale_(optim)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            \n",
    "#             scaler.step(optim)\n",
    "            optim.step()\n",
    "#             scaler.update()\n",
    "            \n",
    "            train_loss += loss.detach()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            del x, y, out, loss, attention_mask\n",
    "            \n",
    "    model_name = type(model).__name__\n",
    "    filename = f'../trained_models/{model_name} Epoch {epoch+1} at {datetime.datetime.now()}'.replace(' ', '_')\n",
    "    with open(filename, 'wb+') as f:\n",
    "        torch.save(model, f)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc1, val_acc10, val_acc100 = 0.0, 0.0, 0.0\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            with tqdm(total=len(dev_loader)) as pbar:\n",
    "                for i, (x, y, wn_ids) in enumerate(dev_loader):\n",
    "                    if i % inc == 0 and i != 0:\n",
    "                        display_loss = val_loss / i\n",
    "                        pbar.set_description(f'Epoch {epoch+1}, Val Loss: {val_loss / i}')\n",
    "\n",
    "                    x = x.to(device)\n",
    "                    attention_mask = (x != train_dataset.pad_id)\n",
    "                    y = y.to(device)\n",
    "                    wn_ids = wn_ids.to_dense().to(device).float()\n",
    "\n",
    "    #                 with autocast():\n",
    "                    loss, out = model(input_ids=x, attention_mask=attention_mask, \n",
    "                                      target_matrix=target_matrix, ground_truth=y,\n",
    "                                      wn_ids=wn_ids, weight_gt=5)\n",
    "\n",
    "                    val_loss += loss.detach()\n",
    "\n",
    "                    pbar.update(1)                \n",
    "\n",
    "                    result, indices = torch.topk(out, k=100, dim=-1, largest=True, sorted=True)\n",
    "\n",
    "                    acc1, acc10, acc100 = evaluate(indices, y)\n",
    "                    val_acc1 += acc1\n",
    "                    val_acc10 += acc10\n",
    "                    val_acc100 += acc100\n",
    "\n",
    "                    del x, y, out, loss\n",
    "    except:\n",
    "        print('Error encountered, aborting validation!')\n",
    "    \n",
    "    wandb.log({\n",
    "        'train_loss': train_loss / len(train_loader),\n",
    "        'val_loss': val_loss / len(dev_loader),\n",
    "        'val_acc1': val_acc1 / len(dev_loader),\n",
    "        'val_acc10': val_acc10 / len(dev_loader),\n",
    "        'val_acc100': val_acc100 / len(dev_loader),\n",
    "        **test(test_loader_seen, 'seen'),\n",
    "        **test(test_loader_unseen, 'unseen'),\n",
    "        **test(test_loader_desc, 'desc')\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredFromDesc(model, desc : str, mask_size=5, top_n=10):\n",
    "    desc = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(desc))\n",
    "    cls_id, mask_id, sep_id, pad_id = train_dataset.cls_id, train_dataset.mask_id, train_dataset.sep_id, train_dataset.pad_id\n",
    "    desc_ids = [cls_id] + [mask_id] * mask_size + [sep_id] + desc\n",
    "    x = torch.tensor(desc_ids).unsqueeze(0).to(device)\n",
    "    attention_mask = (x != pad_id)\n",
    "    out = model(input_ids=x, attention_mask=attention_mask, target_matrix=target_matrix)\n",
    "    result, indices = torch.topk(out, k=top_n, dim=-1, largest=True, sorted=True)\n",
    "    \n",
    "    indices = indices[0]\n",
    "    return [idx2target[i] for i in indices], indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['arctic',\n",
       "  'icy',\n",
       "  'cold',\n",
       "  'glacial',\n",
       "  'coldness',\n",
       "  'frozen',\n",
       "  'coldhearted',\n",
       "  'ice',\n",
       "  'icer',\n",
       "  'rustic',\n",
       "  'cool',\n",
       "  'frost',\n",
       "  'chiller',\n",
       "  'iceman',\n",
       "  'winters',\n",
       "  'european',\n",
       "  'norwegian',\n",
       "  'chill',\n",
       "  'glacier',\n",
       "  'northerner',\n",
       "  'coolie',\n",
       "  'nordic',\n",
       "  'country',\n",
       "  'iceberg',\n",
       "  'icepick',\n",
       "  'coolness',\n",
       "  'barbarian',\n",
       "  'bohemian',\n",
       "  'outflank',\n",
       "  'highlander',\n",
       "  'cooler',\n",
       "  'vagrant',\n",
       "  'polar',\n",
       "  'inhabitant',\n",
       "  'dweller',\n",
       "  'northern',\n",
       "  'alpine',\n",
       "  'scandinavian',\n",
       "  'russian',\n",
       "  'freezing',\n",
       "  'peasant',\n",
       "  'icebound',\n",
       "  'outlander',\n",
       "  'winter',\n",
       "  'gypsy',\n",
       "  'highland',\n",
       "  'deserter',\n",
       "  'chilliness',\n",
       "  'somebody',\n",
       "  'bleak',\n",
       "  'icelandic',\n",
       "  'snowy',\n",
       "  'countryman',\n",
       "  'muscovite',\n",
       "  'irish',\n",
       "  'fragrant',\n",
       "  'continental',\n",
       "  'siberian',\n",
       "  'dwell',\n",
       "  'hardy',\n",
       "  'skiing',\n",
       "  'cooly',\n",
       "  'frosty',\n",
       "  'snow',\n",
       "  'frosting',\n",
       "  'yankee',\n",
       "  'maine',\n",
       "  'icecap',\n",
       "  'blizzard',\n",
       "  'inhabit',\n",
       "  'russ',\n",
       "  'viking',\n",
       "  'aussie',\n",
       "  'groat',\n",
       "  'petticoat',\n",
       "  'skier',\n",
       "  'wild',\n",
       "  'climate',\n",
       "  'subthalamic',\n",
       "  'wilderness',\n",
       "  'shivering',\n",
       "  'desert',\n",
       "  'harsher',\n",
       "  'lurcher',\n",
       "  'barren',\n",
       "  'schnauzer',\n",
       "  'frosted',\n",
       "  'emigrant',\n",
       "  'american',\n",
       "  'scottish',\n",
       "  'swiss',\n",
       "  'goth',\n",
       "  'grizzly',\n",
       "  'cannister',\n",
       "  'traveler',\n",
       "  'soul',\n",
       "  'rusticity',\n",
       "  'furan',\n",
       "  'environmental',\n",
       "  'quagmire'],\n",
       " tensor([16367,  6566, 13156, 35415, 17867, 33742,  8561,    24, 48102, 16493,\n",
       "         17559, 16958, 31285, 29936, 14179, 11250, 45092, 46392, 50017, 27854,\n",
       "         43267, 31187, 21905, 32334, 48356, 40668,  8324,  4860, 22482,  9948,\n",
       "         11401, 10715, 15108, 27729,  8139, 14713, 38866, 23803, 44472, 23980,\n",
       "          8531, 22998, 19714, 26260, 15094, 41112, 15319, 30305,  4859,   249,\n",
       "          5103, 28728, 26280, 30613, 39578, 38576, 21537,  8383, 45077, 45407,\n",
       "         19941, 45556, 42417, 16317, 40335, 31248, 15562, 41832, 36192, 47105,\n",
       "         40416,  8008, 41888, 24059, 20183, 24471, 44151,  2795, 32186, 28341,\n",
       "         25211, 19627, 18225, 17946,  5233, 23923, 39998, 30586, 24167,   112,\n",
       "         44867,  9566, 38891,  3856, 42179, 12017, 27731, 39378, 49621, 38077],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'an inhabitant of a cold country', 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['tree',\n",
       "  'wood',\n",
       "  'treed',\n",
       "  'shrub',\n",
       "  'treetop',\n",
       "  'teakwood',\n",
       "  'pinewood',\n",
       "  'lime',\n",
       "  'linden',\n",
       "  'spruce',\n",
       "  'bush',\n",
       "  'logwood',\n",
       "  'maple',\n",
       "  'chestnut',\n",
       "  'timber',\n",
       "  'birch',\n",
       "  'oak',\n",
       "  'pollard',\n",
       "  'woodcut',\n",
       "  'mahogany',\n",
       "  'hazel',\n",
       "  'woodpecker',\n",
       "  'boxwood',\n",
       "  'conifer',\n",
       "  'bearberry',\n",
       "  'fir',\n",
       "  'hop',\n",
       "  'eucalyptus',\n",
       "  'forest',\n",
       "  'brushwood',\n",
       "  'manoeuver',\n",
       "  'rosewood',\n",
       "  'huckleberry',\n",
       "  'vine',\n",
       "  'laurel',\n",
       "  'pine',\n",
       "  'bark',\n",
       "  'pineapple',\n",
       "  'cottonwood',\n",
       "  'herb',\n",
       "  'raspberry',\n",
       "  'beechwood',\n",
       "  'redwood',\n",
       "  'palm',\n",
       "  'cypress',\n",
       "  'hazelnut',\n",
       "  'plum',\n",
       "  'bushwhack',\n",
       "  'dogwood',\n",
       "  'grow',\n",
       "  'fig',\n",
       "  'tea',\n",
       "  'grass',\n",
       "  'lumber',\n",
       "  'gum',\n",
       "  'logjam',\n",
       "  'ebony',\n",
       "  'bushmeat',\n",
       "  'log',\n",
       "  'cover',\n",
       "  'hit',\n",
       "  'vinifera',\n",
       "  'cedarwood',\n",
       "  'leatherwood',\n",
       "  'sap',\n",
       "  'cordwood',\n",
       "  'berry',\n",
       "  'elderberry',\n",
       "  'bayberry',\n",
       "  'tap',\n",
       "  'elm',\n",
       "  'support',\n",
       "  'shoot',\n",
       "  'touchwood',\n",
       "  'gage',\n",
       "  'hawthorn',\n",
       "  'woodruff',\n",
       "  'willow',\n",
       "  'taproot',\n",
       "  'oakleaf',\n",
       "  'root',\n",
       "  'limes',\n",
       "  'gudgeon',\n",
       "  'hackberry',\n",
       "  'coffee',\n",
       "  'heath',\n",
       "  'barkeep',\n",
       "  'juju',\n",
       "  'make',\n",
       "  'hosanna',\n",
       "  'birchbark',\n",
       "  'cassava',\n",
       "  'basil',\n",
       "  'shrubland',\n",
       "  'peckerwood',\n",
       "  'calabash',\n",
       "  'flora',\n",
       "  'sucker',\n",
       "  'plant',\n",
       "  'firkin'],\n",
       " tensor([19190, 44992,  6880, 43236, 20979,  3398, 22248,  1751, 11683, 30887,\n",
       "         29632, 44267,  4171, 35239, 49479, 20870, 21233, 36534,  7150, 36541,\n",
       "         33988, 41437, 47100,  6640, 47209,  1662, 43217, 25893, 29880,  2813,\n",
       "         28640, 22808, 36789, 22761, 40729, 40429, 47104, 20465,  6686, 47677,\n",
       "          4836, 14218, 11847,  3487, 11498, 14889, 12925, 18219, 41880, 28795,\n",
       "         43825, 22703, 35043, 39458, 21368,  7851, 35378, 49481, 42145, 29635,\n",
       "         35977, 15327, 30973, 25615, 16670, 18392, 42835, 44975, 22244, 33145,\n",
       "         50260, 42581,  4345,  6852, 14722, 42985,  8120, 24594,  4338,  6659,\n",
       "         39037, 12889, 50282, 29005, 20899, 12652,  6438,  4854, 19451, 35658,\n",
       "         46083, 44518,  8322,   296,  9745, 42066,  3294, 30817, 38139, 50431],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, '', 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['road',\n",
       "  'drive',\n",
       "  'runway',\n",
       "  'expressway',\n",
       "  'freeway',\n",
       "  'highway',\n",
       "  'trackway',\n",
       "  'roads',\n",
       "  'move',\n",
       "  'motorway',\n",
       "  'track',\n",
       "  'drag',\n",
       "  'turn',\n",
       "  'driveway',\n",
       "  'route',\n",
       "  'turnpike',\n",
       "  'travel',\n",
       "  'fast',\n",
       "  'parkway',\n",
       "  'beltway',\n",
       "  'speedway',\n",
       "  'pass',\n",
       "  'raceway',\n",
       "  'belt',\n",
       "  'street',\n",
       "  'speed',\n",
       "  'hit',\n",
       "  'motor',\n",
       "  'strip',\n",
       "  'make',\n",
       "  'roadster',\n",
       "  'straightway',\n",
       "  'crisscross',\n",
       "  'render',\n",
       "  'thruway',\n",
       "  'straight',\n",
       "  'autobahn',\n",
       "  'maneuver',\n",
       "  'chase',\n",
       "  'get',\n",
       "  'mean',\n",
       "  'drift',\n",
       "  'go',\n",
       "  'take',\n",
       "  'dragster',\n",
       "  'rip',\n",
       "  'lift',\n",
       "  'range',\n",
       "  'way',\n",
       "  'channel',\n",
       "  'itinerary',\n",
       "  'supply',\n",
       "  'railway',\n",
       "  'incline',\n",
       "  'glide',\n",
       "  'course',\n",
       "  'railroad',\n",
       "  'flyway',\n",
       "  'run',\n",
       "  'path',\n",
       "  'canal',\n",
       "  'trail',\n",
       "  'reach',\n",
       "  'pike',\n",
       "  'thoroughfare',\n",
       "  'avenue',\n",
       "  'draw',\n",
       "  'fly',\n",
       "  'superhighway',\n",
       "  'bypass',\n",
       "  'divide',\n",
       "  'interchange',\n",
       "  'tail',\n",
       "  'zip',\n",
       "  'reverberate',\n",
       "  'straightaway',\n",
       "  'curve',\n",
       "  'rails',\n",
       "  'carry',\n",
       "  'fix',\n",
       "  'getaway',\n",
       "  'cutting',\n",
       "  'section',\n",
       "  'fasting',\n",
       "  'ramp',\n",
       "  'stretch',\n",
       "  'passageway',\n",
       "  'roadkill',\n",
       "  'nose',\n",
       "  'circuit',\n",
       "  'gravel',\n",
       "  'point',\n",
       "  'roadie',\n",
       "  'getter',\n",
       "  'driveline',\n",
       "  'fasten',\n",
       "  'bridge',\n",
       "  'takeaway',\n",
       "  'direct',\n",
       "  'modify'],\n",
       " tensor([18268, 34151, 28832,  2538, 19354, 14565, 31165, 25956, 17382, 49008,\n",
       "         34101, 22501, 31034, 17262, 15165, 13115, 31950,  4241, 35861, 19701,\n",
       "          8640, 24348,  9398, 28174, 12457, 48665, 35977, 22759, 38140, 19451,\n",
       "         15250, 49222,  1906, 13422, 34815, 20965, 23656, 32385, 19823, 20976,\n",
       "         17183, 11066, 15331, 49130, 27828, 36515, 40676, 34219, 10387,  1101,\n",
       "          6690, 36805,  6535, 25059, 18318,  2910, 25796,  7642, 37921, 13145,\n",
       "         13766, 32637,  2799,  4526, 29589, 20113, 26765,  7038, 34796, 17823,\n",
       "           798, 21922, 48432, 38056, 19061,  2050, 43472, 34182, 16202, 12813,\n",
       "         41689,  6369,  3535, 29769, 36404, 45862, 31257,  3827, 49801, 28444,\n",
       "         26344, 23700, 16298, 30896, 26208, 15808,  1521,   310, 30831,  6286],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'a road on which cars can go fast', 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['intellect',\n",
       "  'intelligent',\n",
       "  'genius',\n",
       "  'intelligence',\n",
       "  'intellectual',\n",
       "  'brain',\n",
       "  'somebody',\n",
       "  'brainiac',\n",
       "  'brainpower',\n",
       "  'expert',\n",
       "  'einstein',\n",
       "  'mind',\n",
       "  'intelligentsia',\n",
       "  'person',\n",
       "  'brainstem',\n",
       "  'brains',\n",
       "  'soul',\n",
       "  'minder',\n",
       "  'expertness',\n",
       "  'scientist',\n",
       "  'sensitive',\n",
       "  'wit',\n",
       "  'smart',\n",
       "  'brainy',\n",
       "  'philosopher',\n",
       "  'sensible',\n",
       "  'psychic',\n",
       "  'intellectualism',\n",
       "  'mindfulness',\n",
       "  'brainwashed',\n",
       "  'adept',\n",
       "  'scholar',\n",
       "  'clever',\n",
       "  'eyewitness',\n",
       "  'sensitiveness',\n",
       "  'brainstorming',\n",
       "  'braincase',\n",
       "  'smartness',\n",
       "  'sentient',\n",
       "  'brainchild',\n",
       "  'emollient',\n",
       "  'mastermind',\n",
       "  'intellection',\n",
       "  'visionary',\n",
       "  'individual',\n",
       "  'insight',\n",
       "  'sentience',\n",
       "  'individualist',\n",
       "  'cognition',\n",
       "  'keen',\n",
       "  'mindful',\n",
       "  'adeptness',\n",
       "  'brainwash',\n",
       "  'cleverness',\n",
       "  'creativity',\n",
       "  'think',\n",
       "  'soulmate',\n",
       "  'minding',\n",
       "  'knowledge',\n",
       "  'artist',\n",
       "  'psyche',\n",
       "  'brilliant',\n",
       "  'wisdom',\n",
       "  'soulful',\n",
       "  'intelligently',\n",
       "  'keener',\n",
       "  'sense',\n",
       "  'reader',\n",
       "  'science',\n",
       "  'able',\n",
       "  'sapient',\n",
       "  'exponent',\n",
       "  'judgment',\n",
       "  'intuitive',\n",
       "  'ability',\n",
       "  'thinker',\n",
       "  'dreamer',\n",
       "  'cognitive',\n",
       "  'keenness',\n",
       "  'idiot',\n",
       "  'perceive',\n",
       "  'sharp',\n",
       "  'student',\n",
       "  'intuition',\n",
       "  'fool',\n",
       "  'imaginative',\n",
       "  'intellectually',\n",
       "  'knowing',\n",
       "  'brilliance',\n",
       "  'sympathizer',\n",
       "  'souled',\n",
       "  'faculty',\n",
       "  'learned',\n",
       "  'nerve',\n",
       "  'smarting',\n",
       "  'grown',\n",
       "  'head',\n",
       "  'creative',\n",
       "  'reasoning',\n",
       "  'thought'],\n",
       " tensor([22791, 12896, 27923, 18976, 33895, 38338,  4859, 15336, 24301, 38183,\n",
       "         47707, 30287, 35456, 33190, 43943, 25579, 12017, 32666, 35053,  4512,\n",
       "         35386, 24897, 31720, 22053,  4896, 16946, 50054, 44184,  5316,   312,\n",
       "         38577,  1930, 22441, 29234, 11554, 19970,  9668, 38157,  4612, 30253,\n",
       "         43545,  3733, 22628, 26837, 38983, 46080,  2138, 42289, 10494, 21679,\n",
       "          8616, 18361,  6669, 19335, 18866, 11810, 43320, 36344,  8441, 20681,\n",
       "         21877,  9645,    39, 30759, 27477, 43757, 31496, 20318, 32959, 27766,\n",
       "         29465, 44503, 29770, 27356, 13549, 42798,  3357, 24285, 46807, 13296,\n",
       "            92, 22269, 19574,  4680, 17206, 43034, 23851, 21410, 16159,  8871,\n",
       "          1640, 21082, 48534, 40368,  5311,  4253, 49345, 47681, 41906, 25392],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'an intelligent person', 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.76 GiB total capacity; 12.38 GiB already allocated; 17.75 MiB free; 13.48 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a599a3ed6ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seen'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# epoch 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-c2295e34bad2>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader, name, log)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 loss, out = model(input_ids=x, attention_mask=attention_mask, \n\u001b[1;32m     22\u001b[0m                                   \u001b[0mtarget_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                   wn_ids=wn_ids, weight_gt=5)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dl/11785-Reverse-Dictionary-Project/code/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, target_matrix, ground_truth, sep_id, wn_ids, weight_gt, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# word_scores[batch][mask_size][ww_vocab_size] =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m#       scores[batch][mask_size][target_matrix[batch][mask_size][ww_vocab_size]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mword_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m# (batch, ww_vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# add log probs along mask dim --> equivalent to multiplying probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.76 GiB total capacity; 12.38 GiB already allocated; 17.75 MiB free; 13.48 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "test(test_loader_seen, 'seen') # epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Loss: 105.3856201171875: 100%|██████████| 11/11 [00:01<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen_test_loss: tensor(107.8782, device='cuda:0')\n",
      "unseen_test_acc1: 0.056\n",
      "unseen_test_acc10: 0.216\n",
      "unseen_test_acc100: 0.366\n",
      "unseen_test_rank_median: tensor(640.)\n",
      "unseen_test_rank_variance tensor(458.6854)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unseen_test_loss': tensor(107.8782, device='cuda:0'),\n",
       " 'unseen_test_acc1': 0.056,\n",
       " 'unseen_test_acc10': 0.216,\n",
       " 'unseen_test_acc100': 0.366,\n",
       " 'unseen_test_rank_median': tensor(640.),\n",
       " 'unseen_test_rank_variance': tensor(458.6854)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_unseen, 'unseen') # epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Loss: 199.60968017578125: 100%|██████████| 5/5 [00:00<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_test_loss: tensor(173.0187, device='cuda:0')\n",
      "desc_test_acc1: 0.195\n",
      "desc_test_acc10: 0.705\n",
      "desc_test_acc100: 0.935\n",
      "desc_test_rank_median: tensor(3.)\n",
      "desc_test_rank_variance tensor(88.3132)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'desc_test_loss': tensor(173.0187, device='cuda:0'),\n",
       " 'desc_test_acc1': 0.195,\n",
       " 'desc_test_acc10': 0.705,\n",
       " 'desc_test_acc100': 0.935,\n",
       " 'desc_test_rank_median': tensor(3.),\n",
       " 'desc_test_rank_variance': tensor(88.3132)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_desc, 'desc') # epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../trained_models/epoch2_mid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "metadata": {
   "interpreter": {
    "hash": "049faeb3947c48ac1b8702363c1a3bc597f6c2e1e1396be70d54511d980ab606"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
