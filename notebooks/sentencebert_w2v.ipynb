{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "sys.path.append('../code')\n",
    "from dataset import get_data, make_vocab, WantWordsDataset as WWDataset\n",
    "\n",
    "from transformers import (\n",
    "    AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from models import SentenceBERTForRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "word2vec: 75099 vectors\n",
      "Training data: 675715 word-def pairs\n",
      "Dev data: 75873 word-def pairs\n",
      "Test data: 1200 word-def pairs\n"
     ]
    }
   ],
   "source": [
    "d = get_data('../wantwords-english-baseline/data', word2vec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_data_def, dev_data, test_data_seen, \\\n",
    "    test_data_unseen, test_data_desc, word2vec = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target2idx, idx2target = make_vocab(d[:-1], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16187, 'book')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target2idx maps target words to indices\n",
    "# target_matrix maps target indices to bpe sequences, padded/truncated to mask_size\n",
    "target2idx['book'], idx2target[target2idx['book']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can freeze for (part of) first epoch or so and then unfreeze to train the whole model\n",
    "model = SentenceBERTForRD('distilbert-base-nli-stsb-mean-tokens', \n",
    "                          300, freeze_sbert=False, criterion=nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = model.sbert.tokenizer\n",
    "train_dataset = WWDataset(train_data + train_data_def, T, target2idx, word2vec)\n",
    "dev_dataset = WWDataset(dev_data, T, target2idx, word2vec)\n",
    "test_dataset_seen = WWDataset(test_data_seen, T, target2idx, word2vec)\n",
    "test_dataset_unseen = WWDataset(test_data_unseen, T, target2idx, word2vec)\n",
    "test_dataset_desc = WWDataset(test_data_desc, T, target2idx, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 55\n",
    "num_workers = 0\n",
    "\n",
    "loader_params = {\n",
    "    'pin_memory': False,\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': num_workers,\n",
    "    'collate_fn': train_dataset.collate_fn\n",
    "}\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, **{'shuffle': True, **loader_params})\n",
    "dev_loader = data.DataLoader(dev_dataset, **{'shuffle': True, **loader_params})\n",
    "test_loader_seen = data.DataLoader(test_dataset_seen, **{'shuffle': False, **loader_params})\n",
    "test_loader_unseen = data.DataLoader(test_dataset_unseen, **{'shuffle': False, **loader_params})\n",
    "test_loader_desc = data.DataLoader(test_dataset_desc, **{'shuffle': False, **loader_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "lr = 2e-5\n",
    "optim = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "warmup_duration = 0.01 # portion of the first epoch spent on lr warmup\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=len(train_loader) * warmup_duration, \n",
    "                                            num_training_steps=len(train_loader) * epochs)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreverse-dict\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.29 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">efficient-sunset-90</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/reverse-dict/reverse-dictionary\" target=\"_blank\">https://wandb.ai/reverse-dict/reverse-dictionary</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/reverse-dict/reverse-dictionary/runs/15vhf7ko\" target=\"_blank\">https://wandb.ai/reverse-dict/reverse-dictionary/runs/15vhf7ko</a><br/>\n",
       "                Run data is saved locally in <code>/home/ubuntu/dl/11785-Reverse-Dictionary-Project/notebooks/wandb/run-20210506_064815-15vhf7ko</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f752726c810>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project='reverse-dictionary', entity='reverse-dict')\n",
    "\n",
    "config = wandb.config\n",
    "config.learning_rate = lr\n",
    "config.epochs = epochs\n",
    "config.batch_size = batch_size\n",
    "config.optimizer = type(optim).__name__\n",
    "config.scheduler = type(scheduler).__name__\n",
    "config.warmup_duration = warmup_duration\n",
    "\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "word2vec.embeddings = word2vec.embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred, gt, test=False):\n",
    "    acc1 = acc10 = acc100 = 0\n",
    "    n = len(pred)\n",
    "    pred_rank = []\n",
    "    for p, word in zip(pred, gt):\n",
    "        if test:\n",
    "            loc = (p == word).nonzero(as_tuple=True)\n",
    "            if len(loc) != 0:\n",
    "                pred_rank.append(min(loc[-1], 1000))\n",
    "            else:\n",
    "                pred_rank.append(1000)\n",
    "        if word in p[:100]:\n",
    "            acc100 += 1\n",
    "            if word in p[:10]:\n",
    "                acc10 += 1\n",
    "                if word == p[0]:\n",
    "                    acc1 += 1\n",
    "    if test:\n",
    "        pred_rank = torch.tensor(pred_rank, dtype=torch.float32)\n",
    "        return (acc1, acc10, acc100, pred_rank)\n",
    "    else:\n",
    "        return acc1/n, acc10/n, acc100/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, name):\n",
    "    inc = 3\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc1 = test_acc10 = test_acc100 = 0.0\n",
    "    total_seen = 0\n",
    "    all_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(loader)) as pbar:\n",
    "            for i, ((x, attention_mask), y, yvecs) in enumerate(loader):\n",
    "                if i % inc == 0 and i != 0:\n",
    "                    display_loss = test_loss / i\n",
    "                    pbar.set_description(f'Test Loss: {display_loss}')\n",
    "\n",
    "                x = x.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                y = y.to(device)\n",
    "                yvecs = yvecs.to(device)\n",
    "\n",
    "#                 with autocast():\n",
    "                loss, out = model(input_ids=x, attention_mask=attention_mask,\n",
    "                                  ground_truth=yvecs)\n",
    "\n",
    "                test_loss += loss.detach()\n",
    "            \n",
    "                out = out @ word2vec.embeddings.T\n",
    "\n",
    "                result, indices = torch.sort(out, descending=True)\n",
    "                \n",
    "                b = len(x)\n",
    "                acc1, acc10, acc100, pred_rank = evaluate(indices, y, test=True)\n",
    "                test_acc1 += acc1\n",
    "                test_acc10 += acc10\n",
    "                test_acc100 += acc100\n",
    "                total_seen += b\n",
    "                all_pred.extend(pred_rank)\n",
    "                \n",
    "                del x, y, out, loss\n",
    "                if i % 20 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                pbar.update(1)\n",
    "    \n",
    "    test_loss /= len(loader)\n",
    "    test_acc1 /= total_seen\n",
    "    test_acc10 /= total_seen\n",
    "    test_acc100 /= total_seen\n",
    "    all_pred = torch.tensor(all_pred)\n",
    "    median = torch.median(all_pred)\n",
    "    var = torch.var(all_pred)**0.5\n",
    "    \n",
    "    print(f'{name}_test_loss:', test_loss)\n",
    "    print(f'{name}_test_acc1:', test_acc1)\n",
    "    print(f'{name}_test_acc10:', test_acc10)\n",
    "    print(f'{name}_test_acc100:', test_acc100)\n",
    "    print(f'{name}_test_rank_median:', median)\n",
    "    print(f'{name}_test_rank_variance', var)\n",
    "    \n",
    "    return ({\n",
    "        f'{name}_test_loss': test_loss,\n",
    "        f'{name}_test_acc1': test_acc1,\n",
    "        f'{name}_test_acc10': test_acc10,\n",
    "        f'{name}_test_acc100': test_acc100,\n",
    "        f'{name}_test_rank_median': median,\n",
    "        f'{name}_test_rank_variance': var\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511243f62fd243bdac191cf6d21087a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b645cf44452b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0myvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inc = 10\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epoch, epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    # Train on subset of training data to save time\n",
    "    with tqdm(total=len(train_loader)) as pbar:\n",
    "        for i, ((x, attention_mask), _, yvecs) in enumerate(train_loader):\n",
    "            if i % inc == 0 and i != 0:\n",
    "                display_loss = train_loss / i\n",
    "                pbar.set_description(f'Epoch {epoch+1}, Train Loss: {train_loss / i}')\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            x = x.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            yvecs = yvecs.to(device)\n",
    "            \n",
    "            loss, out = model(input_ids=x, attention_mask=attention_mask, \n",
    "                              ground_truth=yvecs)\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "            loss.backward()\n",
    "            \n",
    "#             scaler.unscale_(optim)\n",
    "            nn.utils.clip_grad_value_(model.parameters(), 5)\n",
    "            \n",
    "#             scaler.step(optim)\n",
    "            optim.step()\n",
    "#             scaler.update()\n",
    "            \n",
    "            train_loss += loss.detach()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            del x, yvecs, out, loss, attention_mask\n",
    "            \n",
    "    model_name = type(model).__name__\n",
    "    filename = f'../trained_models/{model_name} Epoch {epoch+1} at {datetime.datetime.now()}'.replace(' ', '_')\n",
    "    with open(filename, 'wb+') as f:\n",
    "        torch.save({'state_dict': model.state_dict()}, f)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc1, val_acc10, val_acc100 = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(dev_loader)) as pbar:\n",
    "            for i, ((x, attention_mask), y, yvecs) in enumerate(dev_loader):\n",
    "                if i % inc == 0 and i != 0:\n",
    "                    display_loss = val_loss / i\n",
    "                    pbar.set_description(f'Epoch {epoch+1}, Val Loss: {val_loss / i}')\n",
    "\n",
    "                x = x.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                y = y.to(device)\n",
    "                yvecs = yvecs.to(device)\n",
    "\n",
    "#                 with autocast():\n",
    "                loss, out = model(input_ids=x, attention_mask=attention_mask,\n",
    "                                  ground_truth=yvecs)\n",
    "    \n",
    "                out = out @ word2vec.embeddings.T\n",
    "\n",
    "                val_loss += loss.detach()                \n",
    "                \n",
    "                result, indices = torch.topk(out, k=100, dim=-1, largest=True, sorted=True)\n",
    "                \n",
    "                acc1, acc10, acc100 = evaluate(indices, y)\n",
    "                val_acc1 += acc1\n",
    "                val_acc10 += acc10\n",
    "                val_acc100 += acc100\n",
    "\n",
    "                del x, y, yvecs, out, loss\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    wandb.log({\n",
    "        'train_loss': train_loss / len(train_loader),\n",
    "        'val_loss': val_loss / len(dev_loader),\n",
    "        'val_acc1': val_acc1 / len(dev_loader),\n",
    "        'val_acc10': val_acc10 / len(dev_loader),\n",
    "        'val_acc100': val_acc100 / len(dev_loader),\n",
    "        **test(test_loader_seen, 'seen'),\n",
    "        **test(test_loader_unseen, 'unseen'),\n",
    "        **test(test_loader_desc, 'desc')\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredFromDesc(model, desc : str, top_n=10):\n",
    "    model = model.eval()\n",
    "    desc = T(desc, return_tensors='pt', padding=True)\n",
    "    x = desc['input_ids'].to(device)\n",
    "    attention_mask = desc['attention_mask'].to(device)\n",
    "    out = model(input_ids=x, attention_mask=attention_mask)\n",
    "    out = out @ word2vec.embeddings.T\n",
    "    result, indices = torch.topk(out, k=top_n, dim=-1, largest=True, sorted=True)\n",
    "    \n",
    "    indices = indices[0]\n",
    "    return [word2vec.itos[i] for i in indices], indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29b0dba8d3444719ff2d617fe2d992a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_test_loss: tensor(0.0313, device='cuda:0')\n",
      "seen_test_acc1: 0.0\n",
      "seen_test_acc10: 0.0\n",
      "seen_test_acc100: 0.004\n",
      "seen_test_rank_median: tensor(1000.)\n",
      "seen_test_rank_variance tensor(100.8192)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seen_test_loss': tensor(0.0313, device='cuda:0'),\n",
       " 'seen_test_acc1': 0.0,\n",
       " 'seen_test_acc10': 0.0,\n",
       " 'seen_test_acc100': 0.004,\n",
       " 'seen_test_rank_median': tensor(1000.),\n",
       " 'seen_test_rank_variance': tensor(100.8192)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_seen, 'seen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a317254af5a846af8aebcc1cb19c847b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen_test_loss: tensor(14.7876, device='cuda:0')\n",
      "unseen_test_acc1: 0.004\n",
      "unseen_test_acc10: 0.006\n",
      "unseen_test_acc100: 0.006\n",
      "unseen_test_rank_median: tensor(1000.)\n",
      "unseen_test_rank_variance tensor(82.8673)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unseen_test_loss': tensor(14.7876, device='cuda:0'),\n",
       " 'unseen_test_acc1': 0.004,\n",
       " 'unseen_test_acc10': 0.006,\n",
       " 'unseen_test_acc100': 0.006,\n",
       " 'unseen_test_rank_median': tensor(1000.),\n",
       " 'unseen_test_rank_variance': tensor(82.8673)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_unseen, 'unseen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1269fd01ddfc4ebc97db06ae0abad277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_test_loss: tensor(0.0275, device='cuda:0')\n",
      "desc_test_acc1: 0.0\n",
      "desc_test_acc10: 0.0\n",
      "desc_test_acc100: 0.0\n",
      "desc_test_rank_median: tensor(1000.)\n",
      "desc_test_rank_variance tensor(0.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'desc_test_loss': tensor(0.0275, device='cuda:0'),\n",
       " 'desc_test_acc1': 0.0,\n",
       " 'desc_test_acc10': 0.0,\n",
       " 'desc_test_acc100': 0.0,\n",
       " 'desc_test_rank_median': tensor(1000.),\n",
       " 'desc_test_rank_variance': tensor(0.)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_desc, 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['permafrost',\n",
       "  'tundra',\n",
       "  'snowpack',\n",
       "  'glaciers',\n",
       "  'pikas',\n",
       "  'snowing',\n",
       "  'microclimate',\n",
       "  'timbered',\n",
       "  'snows',\n",
       "  'steppe',\n",
       "  'unspoilt',\n",
       "  'alps',\n",
       "  'massif',\n",
       "  'olp',\n",
       "  'steppes',\n",
       "  'showery',\n",
       "  'arctic',\n",
       "  'snowfalls',\n",
       "  'wintry',\n",
       "  'lynx',\n",
       "  'subzero',\n",
       "  'treeless',\n",
       "  'snowfall',\n",
       "  'outdoorsy',\n",
       "  'crevasses',\n",
       "  'morels',\n",
       "  'moors',\n",
       "  'snowfield',\n",
       "  'winters',\n",
       "  'glacier',\n",
       "  'snowy',\n",
       "  'snow',\n",
       "  'glaciation',\n",
       "  'obigation',\n",
       "  'marmot',\n",
       "  'alpine',\n",
       "  'overwinter',\n",
       "  'taiga',\n",
       "  'igloo',\n",
       "  'frost',\n",
       "  'lichen',\n",
       "  'pika',\n",
       "  'ptarmigan',\n",
       "  'frostbitten',\n",
       "  'snowshoes',\n",
       "  'frosts',\n",
       "  'fjord',\n",
       "  'meltwater',\n",
       "  'anticyclone',\n",
       "  'primroses',\n",
       "  'crags',\n",
       "  'tannic',\n",
       "  'crag',\n",
       "  'glades',\n",
       "  'mossy',\n",
       "  'birches',\n",
       "  'subarctic',\n",
       "  'rustic',\n",
       "  'moose',\n",
       "  'hoppy',\n",
       "  'icy',\n",
       "  'subfreezing',\n",
       "  'moister',\n",
       "  'aspen',\n",
       "  'precipitation',\n",
       "  'coldest',\n",
       "  'yurt',\n",
       "  'lichens',\n",
       "  'yeti',\n",
       "  'snowcapped',\n",
       "  'snowshoe',\n",
       "  'colder',\n",
       "  'terroir',\n",
       "  'parka',\n",
       "  'bluebells',\n",
       "  'sleet',\n",
       "  'overwintering',\n",
       "  'malty',\n",
       "  'refrigerate',\n",
       "  'backcountry',\n",
       "  'musher',\n",
       "  'fiords',\n",
       "  'wanderer',\n",
       "  'huckleberries',\n",
       "  'russet',\n",
       "  'deciduous',\n",
       "  'chevre',\n",
       "  'tussock',\n",
       "  'icecap',\n",
       "  'boreal',\n",
       "  'clematis',\n",
       "  'leafless',\n",
       "  'wildflowers',\n",
       "  'skiable',\n",
       "  'wilds',\n",
       "  'snowbound',\n",
       "  'balmy',\n",
       "  'bluebird',\n",
       "  'touristy',\n",
       "  'crocuses'],\n",
       " tensor([40240, 38860, 10792, 50879, 62717, 55803, 18776, 38815, 51019,  5448,\n",
       "          7924, 34904, 20634, 70529, 74567, 24201,  6008, 54619, 65200, 26320,\n",
       "         37141, 41793, 24493, 29902, 59489, 63310, 25877, 20229, 35748, 45641,\n",
       "         48451, 47659, 29526, 71114, 33227, 31794, 46384, 20443, 40280, 38071,\n",
       "         11837, 71640, 38549, 45110, 73582, 67089, 60654, 26327, 35237, 50887,\n",
       "         67588, 30224, 35875, 14476, 11451, 70402, 23435, 14309, 27217, 32716,\n",
       "          7600, 52689, 60701, 25966, 62776, 51636, 73547, 72473, 39447, 53093,\n",
       "          2588, 64039, 36216, 18114, 45658, 46002, 33509, 48028, 40976, 49455,\n",
       "          6430, 58447, 68366, 67392, 37074, 18360,  7066, 49957, 37389,  4932,\n",
       "         15484, 23610, 72968, 48095, 68784,  5287, 19425, 22209, 28718, 57731],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'an inhabitant of a cold country', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['panto',\n",
       "  'vaudeville',\n",
       "  'ceilidh',\n",
       "  'pantomime',\n",
       "  'tearoom',\n",
       "  'governess',\n",
       "  'busker',\n",
       "  'funfair',\n",
       "  'projectionist',\n",
       "  'cabaret',\n",
       "  'speakeasy',\n",
       "  'vaudevillian',\n",
       "  'burlesque',\n",
       "  'taxidermist',\n",
       "  'stagehand',\n",
       "  'nobleman',\n",
       "  'minstrel',\n",
       "  'ventriloquist',\n",
       "  'toff',\n",
       "  'bullfight',\n",
       "  'taxidermy',\n",
       "  'showgirl',\n",
       "  'costuming',\n",
       "  'puppeteer',\n",
       "  'boardinghouse',\n",
       "  'proscenium',\n",
       "  'aerialist',\n",
       "  'marionette',\n",
       "  'taverna',\n",
       "  'magician',\n",
       "  'geisha',\n",
       "  'footman',\n",
       "  'milliner',\n",
       "  'carny',\n",
       "  'garret',\n",
       "  'madwoman',\n",
       "  'circus',\n",
       "  'stagehands',\n",
       "  'cowhand',\n",
       "  'seance',\n",
       "  'manservant',\n",
       "  'noblewoman',\n",
       "  'kabuki',\n",
       "  'blacksmith',\n",
       "  'circuses',\n",
       "  'bordello',\n",
       "  'pantomimes',\n",
       "  'fayre',\n",
       "  'trattoria',\n",
       "  'juggler',\n",
       "  'showman',\n",
       "  'ventriloquism',\n",
       "  'séance',\n",
       "  'bullfights',\n",
       "  'nunnery',\n",
       "  'mime',\n",
       "  'schoolmaster',\n",
       "  'wench',\n",
       "  'huntsman',\n",
       "  'troupe',\n",
       "  'dowager',\n",
       "  'trapeze',\n",
       "  'scherzo',\n",
       "  'hostelry',\n",
       "  'haberdashery',\n",
       "  'coloratura',\n",
       "  'marionettes',\n",
       "  'nativity',\n",
       "  'roadhouse',\n",
       "  'boozer',\n",
       "  'seamstress',\n",
       "  'fairground',\n",
       "  'animatronics',\n",
       "  'crone',\n",
       "  'stationmaster',\n",
       "  'bullfighter',\n",
       "  'brothel',\n",
       "  'operetta',\n",
       "  'shepherdess',\n",
       "  'smokehouse',\n",
       "  'gondolier',\n",
       "  'fiddler',\n",
       "  'emporium',\n",
       "  'stagecoach',\n",
       "  'greengrocer',\n",
       "  'fishmonger',\n",
       "  'brasserie',\n",
       "  'woodsman',\n",
       "  'accordionist',\n",
       "  'hypnotist',\n",
       "  'sheepdog',\n",
       "  'dotty',\n",
       "  'illusionist',\n",
       "  'portraitist',\n",
       "  'gamekeeper',\n",
       "  'jugglers',\n",
       "  'nightie',\n",
       "  'puppetry',\n",
       "  'squire',\n",
       "  'innkeeper'],\n",
       " tensor([38283,  6931, 41176, 26119,  3991,  5571, 48760, 33148, 13799, 37942,\n",
       "         30745, 45565, 25265, 31443, 19012, 22906, 25243, 56898, 40970,  9847,\n",
       "         19759, 49612, 63542, 70325,  3174, 37684,  5291, 15201, 46505, 15306,\n",
       "         37555, 39263, 26437, 11690, 11701, 40543, 19858, 57037,  1967, 48544,\n",
       "         16190,   353, 42872, 30324, 55164,  9844, 56049, 35518, 11853, 43684,\n",
       "         19038,  3462, 61624, 69247,    35, 30165, 69337, 62895, 13582, 27781,\n",
       "          2523, 29775, 34692, 17619, 38722, 67326, 55664, 40676, 21891, 20005,\n",
       "         34201,  6435, 43083, 30932, 11924, 16313, 15284, 34061, 39307, 26646,\n",
       "         12871, 23373,  1900, 39105,  6572, 55534, 40126, 57771, 28138, 36395,\n",
       "         21228, 37328, 11033, 15087,  1404, 58486, 22268, 27240, 48795, 37497],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'employee at a circus', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['chicane',\n",
       "  'motorcade',\n",
       "  'understeer',\n",
       "  'downforce',\n",
       "  'oversteer',\n",
       "  'cortege',\n",
       "  'understeering',\n",
       "  'laps',\n",
       "  'speedometer',\n",
       "  'swerved',\n",
       "  'hearse',\n",
       "  'autocross',\n",
       "  'eastbound',\n",
       "  'suplex',\n",
       "  'southbound',\n",
       "  'stoplight',\n",
       "  'braking',\n",
       "  'mph',\n",
       "  'swerving',\n",
       "  'blinker',\n",
       "  'fishtailed',\n",
       "  'esses',\n",
       "  'northbound',\n",
       "  'jeep',\n",
       "  'swerve',\n",
       "  'hairpin',\n",
       "  'wheelbase',\n",
       "  'freeway',\n",
       "  'dragster',\n",
       "  'carriageway',\n",
       "  'motorcyclist',\n",
       "  'lapper',\n",
       "  'fishtail',\n",
       "  'fastball',\n",
       "  'backstretch',\n",
       "  'lanes',\n",
       "  'speeder',\n",
       "  'guardrail',\n",
       "  'driverless',\n",
       "  'windscreen',\n",
       "  'rockpile',\n",
       "  'headlight',\n",
       "  'cruiser',\n",
       "  'convoy',\n",
       "  'lane',\n",
       "  'superspeedway',\n",
       "  'sunroof',\n",
       "  'sedan',\n",
       "  'layup',\n",
       "  'headlights',\n",
       "  'expressway',\n",
       "  'dragstrip',\n",
       "  'camber',\n",
       "  'jackknifed',\n",
       "  'criterium',\n",
       "  'kart',\n",
       "  'honked',\n",
       "  'carjack',\n",
       "  'grounder',\n",
       "  'contraflow',\n",
       "  'streamliner',\n",
       "  'straightaway',\n",
       "  'motorway',\n",
       "  'gearshift',\n",
       "  'powerslam',\n",
       "  'roadway',\n",
       "  'taxied',\n",
       "  'jaywalk',\n",
       "  'oversteering',\n",
       "  'rickshaw',\n",
       "  'taxiway',\n",
       "  'lap',\n",
       "  'autobahn',\n",
       "  'forehand',\n",
       "  'powerbomb',\n",
       "  'motorbike',\n",
       "  'handbrake',\n",
       "  'windshield',\n",
       "  'snowplow',\n",
       "  'car',\n",
       "  'radioed',\n",
       "  'suv',\n",
       "  'overdriving',\n",
       "  'paps',\n",
       "  'freeways',\n",
       "  'maglev',\n",
       "  'undercarriage',\n",
       "  'sped',\n",
       "  'moped',\n",
       "  'motorcycle',\n",
       "  'driveshaft',\n",
       "  'gearbox',\n",
       "  'pedestrians',\n",
       "  'speedo',\n",
       "  'embankment',\n",
       "  'hydroplanes',\n",
       "  'bicyclist',\n",
       "  'slalom',\n",
       "  'limousine',\n",
       "  'motorist'],\n",
       " tensor([ 6478, 23919, 27162, 36174, 35730, 26525, 51363, 69659, 32151, 74946,\n",
       "         69508, 49854,  6912, 42498, 45828, 24817, 28114, 73561, 11864, 16517,\n",
       "         51829, 36386, 36612, 30599, 19254,  2268, 73195, 27139,  2278, 50335,\n",
       "         57032, 52024, 42548, 12462, 27525, 36288, 34110, 49650, 31664, 64315,\n",
       "         36468, 37725, 11474, 15350, 75028, 28070, 24560, 47284, 34728, 21574,\n",
       "         28635,  7994, 17098, 70602, 25315, 30527, 63390, 68729,   773, 13531,\n",
       "          5852,  6028, 21805, 18415, 35741, 63379, 64123, 23759, 58488, 28505,\n",
       "          7340, 37318, 22163,   789, 46948, 41746, 44824, 74314, 34001, 21782,\n",
       "         73616,  3626, 70698, 65678, 59366, 31551, 52132, 52841, 36392, 31761,\n",
       "         56510, 21065, 59081,  9041, 70825, 55210, 70878, 68769, 16047, 20749],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'a road on which cars can go fast', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['millibars',\n",
       "  'refrigerate',\n",
       "  'temperature',\n",
       "  'thermometer',\n",
       "  'preheat',\n",
       "  'thermostat',\n",
       "  'temperatures',\n",
       "  'thermometers',\n",
       "  'oven',\n",
       "  'thermocouple',\n",
       "  'convection',\n",
       "  'troposphere',\n",
       "  'dehumidifier',\n",
       "  'humidifier',\n",
       "  'altimeter',\n",
       "  'defrost',\n",
       "  'psi',\n",
       "  'hygrometer',\n",
       "  'saucepan',\n",
       "  'conductivity',\n",
       "  'magma',\n",
       "  'snowpack',\n",
       "  'deg',\n",
       "  'thermocouples',\n",
       "  'ionization',\n",
       "  'precipitation',\n",
       "  'humidity',\n",
       "  'evaporator',\n",
       "  'albedo',\n",
       "  'voltages',\n",
       "  'caramelize',\n",
       "  'heater',\n",
       "  'moisture',\n",
       "  'ozone',\n",
       "  'supercooled',\n",
       "  'pressurization',\n",
       "  'defroster',\n",
       "  'coldest',\n",
       "  'skillet',\n",
       "  'anemometer',\n",
       "  'meltwater',\n",
       "  'tsp',\n",
       "  'spectrometer',\n",
       "  'reflectance',\n",
       "  'refrigerating',\n",
       "  'viscosity',\n",
       "  'radon',\n",
       "  'hydrometer',\n",
       "  'inductance',\n",
       "  'sunspot',\n",
       "  'climatologist',\n",
       "  'flowmeter',\n",
       "  'coolant',\n",
       "  'argon',\n",
       "  'watts',\n",
       "  'airflow',\n",
       "  'qubit',\n",
       "  'workpiece',\n",
       "  'warmest',\n",
       "  'refrigerant',\n",
       "  'microclimate',\n",
       "  'conductance',\n",
       "  'condenser',\n",
       "  'airspeed',\n",
       "  'evapotranspiration',\n",
       "  'reflectivity',\n",
       "  'snowmelt',\n",
       "  'reheat',\n",
       "  'convective',\n",
       "  'resistivity',\n",
       "  'subfreezing',\n",
       "  'magnetization',\n",
       "  'spectrophotometer',\n",
       "  'heaters',\n",
       "  'thermocline',\n",
       "  'humidification',\n",
       "  'seismograph',\n",
       "  'photometer',\n",
       "  'exoplanet',\n",
       "  'thrusters',\n",
       "  'permafrost',\n",
       "  'heatsink',\n",
       "  'overcook',\n",
       "  'streamflow',\n",
       "  'ionosphere',\n",
       "  'turbidity',\n",
       "  'downforce',\n",
       "  'tablespoons',\n",
       "  'multimeter',\n",
       "  'seismographs',\n",
       "  'refraction',\n",
       "  'superconducting',\n",
       "  'brightness',\n",
       "  'centigrade',\n",
       "  'saute',\n",
       "  'seismicity',\n",
       "  'downhole',\n",
       "  'redshift',\n",
       "  'quakes',\n",
       "  'anticyclone'],\n",
       " tensor([59319, 40976, 44697,  7868,  8505,  2001, 72762, 50611, 13288, 46746,\n",
       "         70649, 51972, 54215,  6923, 74139, 42568,  4678, 44965, 49343,  3078,\n",
       "         39102, 10792,  7727, 69853, 43833, 62776, 57795, 29857, 34308, 58431,\n",
       "         14933, 10556, 23777,  2801, 42069, 29673, 18547, 51636, 23988, 34736,\n",
       "         26327, 32985, 27027,  3470, 40815, 43397, 23059,  3347, 21196,  9137,\n",
       "         28167, 73683, 60382, 36986, 51269,   290,  6286, 68938, 57165, 41042,\n",
       "         18776, 13795, 11899, 66386, 31458,  2641, 26436, 22379, 24687, 73342,\n",
       "         52689,  1542, 28790, 65208, 25968, 52069,  8156, 15447, 19407, 70851,\n",
       "         40240, 11322, 23552, 21436, 31788, 27229, 36174, 61485,  4213, 58473,\n",
       "         18965, 46786, 20593, 23791, 10361, 21733, 10503,  2430, 66788, 35237],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'something you use to measure your temperature', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['mansion',\n",
       "  'house',\n",
       "  'hall',\n",
       "  'villa',\n",
       "  'palace',\n",
       "  'stateroom',\n",
       "  'apartment',\n",
       "  'hotel',\n",
       "  'homestead',\n",
       "  'domiciled',\n",
       "  'tenement',\n",
       "  'lodge',\n",
       "  'roof',\n",
       "  'lofting',\n",
       "  'casa',\n",
       "  'housing',\n",
       "  'saloon',\n",
       "  'cottage',\n",
       "  'penthouse',\n",
       "  'residence',\n",
       "  'roofs',\n",
       "  'cabinet',\n",
       "  'householder',\n",
       "  'home',\n",
       "  'inn',\n",
       "  'castle',\n",
       "  'maisonette',\n",
       "  'condominium',\n",
       "  'manor',\n",
       "  'seraglio',\n",
       "  'summerhouse',\n",
       "  'nesting',\n",
       "  'placed',\n",
       "  'ibn',\n",
       "  'townhouse',\n",
       "  'parkour',\n",
       "  'cribbed',\n",
       "  'demesne',\n",
       "  'palatial',\n",
       "  'landlord',\n",
       "  'dwelling',\n",
       "  'divan',\n",
       "  'loge',\n",
       "  'parlor',\n",
       "  'court',\n",
       "  'stage',\n",
       "  'nested',\n",
       "  'lodging',\n",
       "  'cabin',\n",
       "  'chateau',\n",
       "  'building',\n",
       "  'nest',\n",
       "  'chamber',\n",
       "  'cloakroom',\n",
       "  'edifice',\n",
       "  'housemate',\n",
       "  'rooms',\n",
       "  'stacked',\n",
       "  'residency',\n",
       "  'buttery',\n",
       "  'guesthouse',\n",
       "  'studio',\n",
       "  'rotunda',\n",
       "  'pension',\n",
       "  'stack',\n",
       "  'floor',\n",
       "  'houseguest',\n",
       "  'treasury',\n",
       "  'kitty',\n",
       "  'hovel',\n",
       "  'lobbying',\n",
       "  'pavilion',\n",
       "  'household',\n",
       "  'condo',\n",
       "  'tenancy',\n",
       "  'roundhouse',\n",
       "  'stacks',\n",
       "  'feature',\n",
       "  'den',\n",
       "  'palazzo',\n",
       "  'hostelry',\n",
       "  'hostel',\n",
       "  'lodgment',\n",
       "  'boudoir',\n",
       "  'housekeeping',\n",
       "  'windsor',\n",
       "  'habitation',\n",
       "  'boardinghouse',\n",
       "  'mahal',\n",
       "  'auditorium',\n",
       "  'crib',\n",
       "  'suite',\n",
       "  'curtilage',\n",
       "  'tard',\n",
       "  'fireside',\n",
       "  'dollhouse',\n",
       "  'gazebo',\n",
       "  'roo',\n",
       "  'atrium',\n",
       "  'terracing'],\n",
       " tensor([22986,  7128,  8017, 44033, 32922, 21396, 31671,  3182, 23876, 39703,\n",
       "         39815, 19985, 22464, 15400, 36882, 23459,  3541, 19213, 28893, 31387,\n",
       "          3719, 27133, 22996, 19909, 39571, 26177, 40618, 30329, 37213, 41353,\n",
       "         17996, 36995, 40966, 17022, 19977, 16961, 31268, 31617, 32515, 27951,\n",
       "          6352, 35749, 34024, 38934, 29739, 39891, 30905, 19808, 39982,  8330,\n",
       "          9694, 10830, 18821, 21574, 11741, 43430,   473,  8358, 15256, 38575,\n",
       "         41475, 31925, 40116,  6013, 31239, 28095, 11411, 11268,  9528, 18040,\n",
       "         32757, 31078, 41851, 11387,   277,  3464, 14390, 12789,  5498,  3538,\n",
       "          3250,  5181, 27225,  5920, 27176, 30281, 36012,  6484,  4942, 28013,\n",
       "         13654, 17704, 13225, 22550, 27454, 23362,  1663, 16222, 29066, 24710],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'a large house that a rich person lives in', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['unhappy',\n",
       "  'happiness',\n",
       "  'sadness',\n",
       "  'misery',\n",
       "  'vanity',\n",
       "  'misfortune',\n",
       "  'bashfulness',\n",
       "  'complacence',\n",
       "  'felicitate',\n",
       "  'evil',\n",
       "  'nothingness',\n",
       "  'depression',\n",
       "  'unhappiness',\n",
       "  'absurd',\n",
       "  'emptiness',\n",
       "  'disappointment',\n",
       "  'frailty',\n",
       "  'pessimism',\n",
       "  'felicity',\n",
       "  'diffidence',\n",
       "  'inconvenience',\n",
       "  'melancholic',\n",
       "  'egoism',\n",
       "  'indisposition',\n",
       "  'irrationality',\n",
       "  'penury',\n",
       "  'folly',\n",
       "  'mischief',\n",
       "  'languor',\n",
       "  'blithe',\n",
       "  'gravity',\n",
       "  'hopeless',\n",
       "  'unreality',\n",
       "  'atheism',\n",
       "  'joviality',\n",
       "  'unreasoning',\n",
       "  'luck',\n",
       "  'adversity',\n",
       "  'egotism',\n",
       "  'discontent',\n",
       "  'disconsolate',\n",
       "  'nihilism',\n",
       "  'insanity',\n",
       "  'pitiable',\n",
       "  'unkindly',\n",
       "  'spleen',\n",
       "  'transience',\n",
       "  'arrogance',\n",
       "  'magnanimity',\n",
       "  'affliction',\n",
       "  'gaiety',\n",
       "  'demerit',\n",
       "  'infatuation',\n",
       "  'modesty',\n",
       "  'ugliness',\n",
       "  'uneasiness',\n",
       "  'impotence',\n",
       "  'disinterest',\n",
       "  'disaffection',\n",
       "  'malevolence',\n",
       "  'sterility',\n",
       "  'flatness',\n",
       "  'envy',\n",
       "  'deficiency',\n",
       "  'goodness',\n",
       "  'deadness',\n",
       "  'regrets',\n",
       "  'leniency',\n",
       "  'joy',\n",
       "  'incontinence',\n",
       "  'happy',\n",
       "  'admiration',\n",
       "  'drought',\n",
       "  'lugubrious',\n",
       "  'humor',\n",
       "  'unfortunate',\n",
       "  'scarcity',\n",
       "  'desperation',\n",
       "  'megalomania',\n",
       "  'sorrowing',\n",
       "  'inertness',\n",
       "  'inanition',\n",
       "  'fatuity',\n",
       "  'closeness',\n",
       "  'nostalgia',\n",
       "  'unlikeliness',\n",
       "  'oll',\n",
       "  'thankless',\n",
       "  'narcissism',\n",
       "  'morbidity',\n",
       "  'distemper',\n",
       "  'want',\n",
       "  'sarcasm',\n",
       "  'disease',\n",
       "  'miracle',\n",
       "  'imperfection',\n",
       "  'photophobia',\n",
       "  'pathetic',\n",
       "  'preposterous',\n",
       "  'disappointing'],\n",
       " tensor([11144, 29088,   128, 40591,  9345, 35444,  1840,  2418, 23682, 39768,\n",
       "         34022, 11426, 31810, 30906, 25108, 34624, 27262, 33645, 24733,  8624,\n",
       "          5069, 13000, 38568, 41362, 38744, 36094,  3090, 25364, 10519,  2541,\n",
       "         16342,  2026, 10185, 20765, 26728, 30546, 34288,  5745, 11322,  3505,\n",
       "         33076, 36922, 23349, 33405, 28585,  8676, 16918,  5749, 22506, 21614,\n",
       "          2748, 16385,  8886, 37310,  8731, 43117, 36663, 32590, 37006, 17912,\n",
       "         17806,  4237, 38816, 32370,  8190,  3087,  2716, 39482, 25831, 31524,\n",
       "         33673, 23635, 30961, 25192, 16657, 39116,  8256, 32211, 27196, 14670,\n",
       "         30120,  8807, 35312,  8976, 20131, 23536, 34601,  4937,  5131, 26060,\n",
       "         40549, 34607, 11040, 21542, 15581, 18317,  5855, 11801,  1727, 23848],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'the opposite of being happy', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['amphibian',\n",
       "  'amphibious',\n",
       "  'swim',\n",
       "  'porpoise',\n",
       "  'plankton',\n",
       "  'aqueous',\n",
       "  'swimmer',\n",
       "  'subaquatic',\n",
       "  'cetacean',\n",
       "  'gill',\n",
       "  'puffer',\n",
       "  'dipper',\n",
       "  'swimming',\n",
       "  'benthos',\n",
       "  'sucker',\n",
       "  'monkfish',\n",
       "  'pelagic',\n",
       "  'underwater',\n",
       "  'hydrate',\n",
       "  'smelt',\n",
       "  'prawn',\n",
       "  'submersible',\n",
       "  'brine',\n",
       "  'salamander',\n",
       "  'ocean',\n",
       "  'limnology',\n",
       "  'beluga',\n",
       "  'leviathan',\n",
       "  'groundfish',\n",
       "  'laver',\n",
       "  'leeching',\n",
       "  'catfish',\n",
       "  'goby',\n",
       "  'porgy',\n",
       "  'neptune',\n",
       "  'pinniped',\n",
       "  'diver',\n",
       "  'hydraulic',\n",
       "  'lifeguard',\n",
       "  'hydrosphere',\n",
       "  'beaver',\n",
       "  'skater',\n",
       "  'drowned',\n",
       "  'bathe',\n",
       "  'sunfish',\n",
       "  'dowse',\n",
       "  'mermaid',\n",
       "  'mussel',\n",
       "  'tadpole',\n",
       "  'dogfish',\n",
       "  'awash',\n",
       "  'sheepshead',\n",
       "  'hardhead',\n",
       "  'grouper',\n",
       "  'planktonic',\n",
       "  'anglerfish',\n",
       "  'jellyfish',\n",
       "  'bloodsucker',\n",
       "  'afloat',\n",
       "  'vessel',\n",
       "  'paddling',\n",
       "  'moray',\n",
       "  'sea',\n",
       "  'darter',\n",
       "  'loach',\n",
       "  'cataract',\n",
       "  'shovelnose',\n",
       "  'mullet',\n",
       "  'swordfish',\n",
       "  'laker',\n",
       "  'butterfish',\n",
       "  'nautilus',\n",
       "  'steamer',\n",
       "  'lake',\n",
       "  'submarine',\n",
       "  'supernatant',\n",
       "  'fount',\n",
       "  'hydroplane',\n",
       "  'floater',\n",
       "  'siren',\n",
       "  'seagoing',\n",
       "  'grayling',\n",
       "  'water',\n",
       "  'hydrography',\n",
       "  'dabbling',\n",
       "  'oyster',\n",
       "  'remora',\n",
       "  'blower',\n",
       "  'marine',\n",
       "  'hydra',\n",
       "  'bream',\n",
       "  'preserver',\n",
       "  'kingfish',\n",
       "  'bonefish',\n",
       "  'ich',\n",
       "  'hydrotherapy',\n",
       "  'butterflyfish',\n",
       "  'hellbender',\n",
       "  'seine',\n",
       "  'reptile'],\n",
       " tensor([10461, 41026, 29554, 38462,   187,   713, 22475, 25763, 44034, 26546,\n",
       "         29289, 37968, 25233, 37034, 30817, 16134, 17623, 25340,  2975,  7530,\n",
       "         15919, 18791,  9003, 35798, 39557, 34223,   414, 17685,  1813, 10265,\n",
       "         16140, 26380, 11176, 33601, 24622, 30723, 20665, 42052, 37759, 18372,\n",
       "          9410, 27672, 35604, 38410,  4219,  8950, 11808,  2112, 14706, 13560,\n",
       "         30705, 36329, 22947, 13670,  7303, 14764, 21745, 26534, 36371, 36778,\n",
       "         18966, 31886, 24709,  3333,   413, 13603, 22962, 17127, 16624,  6358,\n",
       "          3550, 42974, 13395,  2935,  5413,  9973, 32088, 27698, 34064, 25441,\n",
       "         32752, 28062, 30158, 26556, 24021, 36700, 31382, 11628, 20569,  6245,\n",
       "         22889, 22731, 11848,   348,   620,  2200, 36231, 18810,  3533, 26876],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'a mammal that lives in water', 100) # decent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['oceanic',\n",
       "  'ocean',\n",
       "  'pelagic',\n",
       "  'neptune',\n",
       "  'sea',\n",
       "  'benthos',\n",
       "  'oceanography',\n",
       "  'marine',\n",
       "  'leviathan',\n",
       "  'transoceanic',\n",
       "  'seagoing',\n",
       "  'nautilus',\n",
       "  'coaster',\n",
       "  'seabird',\n",
       "  'shearwater',\n",
       "  'beluga',\n",
       "  'beachcomber',\n",
       "  'gam',\n",
       "  'underwater',\n",
       "  'oceangoing',\n",
       "  'prawn',\n",
       "  'narwhal',\n",
       "  'monkfish',\n",
       "  'kingfish',\n",
       "  'bowhead',\n",
       "  'subaquatic',\n",
       "  'porpoise',\n",
       "  'transatlantic',\n",
       "  'shovelnose',\n",
       "  'seashell',\n",
       "  'plankton',\n",
       "  'halibut',\n",
       "  'kelp',\n",
       "  'seaweed',\n",
       "  'tsunami',\n",
       "  'cetacean',\n",
       "  'seahorse',\n",
       "  'hake',\n",
       "  'seaboard',\n",
       "  'mussel',\n",
       "  'archipelago',\n",
       "  'blackfish',\n",
       "  'gopher',\n",
       "  'brine',\n",
       "  'reefer',\n",
       "  'jason',\n",
       "  'iceberg',\n",
       "  'main',\n",
       "  'anchovy',\n",
       "  'buoy',\n",
       "  'submarine',\n",
       "  'seafaring',\n",
       "  'hardhead',\n",
       "  'brachiopod',\n",
       "  'manatee',\n",
       "  'atlantic',\n",
       "  'mermaid',\n",
       "  'baleen',\n",
       "  'groundfish',\n",
       "  'planktonic',\n",
       "  'orion',\n",
       "  'amphibian',\n",
       "  'seaward',\n",
       "  'offshore',\n",
       "  'oversea',\n",
       "  'cancer',\n",
       "  'beagle',\n",
       "  'chiton',\n",
       "  'tide',\n",
       "  'dogfish',\n",
       "  'serval',\n",
       "  'hydrosphere',\n",
       "  'moray',\n",
       "  'lobster',\n",
       "  'crustacean',\n",
       "  'cisco',\n",
       "  'porgy',\n",
       "  'shipwreck',\n",
       "  'barracuda',\n",
       "  'circumnavigation',\n",
       "  'submersible',\n",
       "  'hydra',\n",
       "  'marlin',\n",
       "  'afloat',\n",
       "  'amphibious',\n",
       "  'skinned',\n",
       "  'lithosphere',\n",
       "  'squid',\n",
       "  'plaice',\n",
       "  'butterflyfish',\n",
       "  'oyster',\n",
       "  'cooter',\n",
       "  'overseas',\n",
       "  'circumpolar',\n",
       "  'salter',\n",
       "  'gribble',\n",
       "  'bigfoot',\n",
       "  'asthenosphere',\n",
       "  'kraken',\n",
       "  'bobcat'],\n",
       " tensor([ 1858, 39557, 17623, 24622, 24709, 37034,  7241, 20569, 17685,  3277,\n",
       "         32752, 42974, 28224, 38632,   317,   414, 41624, 13067, 25340, 42401,\n",
       "         15919, 13176, 16134, 11848, 40689, 25763, 38462,  7196, 22962, 37590,\n",
       "           187, 39673, 21816, 14017, 35580, 44034, 39346,  3917,   245,  2112,\n",
       "          4600, 21931,  8370,  9003, 26414,   282, 32334, 24005,   747, 16556,\n",
       "          5413, 19017, 22947,  1986,  8678,  3484, 11808, 14418,  1813,  7303,\n",
       "         41885, 10461, 36501, 30314, 34020, 21541, 36393, 16084, 19842, 13560,\n",
       "         23479, 18372, 31886,  2585, 11170,  9089, 33601, 40640, 39538,  1585,\n",
       "         18791,  6245, 17490, 36371, 41026, 34975,  8550,  8177,  4691, 36231,\n",
       "         36700, 42343, 32456, 15673, 20869, 13305, 27717, 31945, 36578, 34105],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'a mammal that lives in ocean', 100) # bad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['scholar',\n",
       "  'pedant',\n",
       "  'student',\n",
       "  'expert',\n",
       "  'professor',\n",
       "  'intellectual',\n",
       "  'literate',\n",
       "  'adept',\n",
       "  'polyglot',\n",
       "  'bookworm',\n",
       "  'lector',\n",
       "  'educator',\n",
       "  'thinker',\n",
       "  'monitor',\n",
       "  'teacher',\n",
       "  'master',\n",
       "  'virtuoso',\n",
       "  'highbrow',\n",
       "  'reader',\n",
       "  'proficient',\n",
       "  'mastering',\n",
       "  'doctoring',\n",
       "  'knowledgeable',\n",
       "  'skeptic',\n",
       "  'amateur',\n",
       "  'connoisseur',\n",
       "  'pedagogue',\n",
       "  'technocrat',\n",
       "  'doctored',\n",
       "  'savant',\n",
       "  'scientist',\n",
       "  'bookman',\n",
       "  'philosopher',\n",
       "  'erudition',\n",
       "  'conversant',\n",
       "  'doctor',\n",
       "  'gnostic',\n",
       "  'cpa',\n",
       "  'appreciator',\n",
       "  'humanist',\n",
       "  'collector',\n",
       "  'pundit',\n",
       "  'learning',\n",
       "  'lettered',\n",
       "  'mandarin',\n",
       "  'science',\n",
       "  'informant',\n",
       "  'generalist',\n",
       "  'artist',\n",
       "  'scholastic',\n",
       "  'teller',\n",
       "  'learner',\n",
       "  'informer',\n",
       "  'tea',\n",
       "  'technologist',\n",
       "  'inquisitor',\n",
       "  'educated',\n",
       "  'lore',\n",
       "  'fellow',\n",
       "  'orator',\n",
       "  'university',\n",
       "  'specialist',\n",
       "  'technician',\n",
       "  'sage',\n",
       "  'brains',\n",
       "  'stargazer',\n",
       "  'scholarship',\n",
       "  'intellect',\n",
       "  'study',\n",
       "  'exegete',\n",
       "  'empiric',\n",
       "  'observer',\n",
       "  'mistress',\n",
       "  'prophet',\n",
       "  'studied',\n",
       "  'herbalist',\n",
       "  'faculty',\n",
       "  'experienced',\n",
       "  'instructor',\n",
       "  'textbook',\n",
       "  'monitoring',\n",
       "  'ace',\n",
       "  'crammer',\n",
       "  'speller',\n",
       "  'reckoner',\n",
       "  'bookish',\n",
       "  'literacy',\n",
       "  'viewer',\n",
       "  'server',\n",
       "  'teachable',\n",
       "  'mathematician',\n",
       "  'logician',\n",
       "  'autodidact',\n",
       "  'analyst',\n",
       "  'boffin',\n",
       "  'exponent',\n",
       "  'skilled',\n",
       "  'computer',\n",
       "  'virtuosity',\n",
       "  'bibliophile'],\n",
       " tensor([ 1930, 19001, 19574, 38183,  2972, 33895, 34372, 38577, 19298, 43513,\n",
       "         43688, 29485, 42798, 10077,  5177, 40203, 37772, 29362, 20318, 26913,\n",
       "         28376, 17576, 21099, 44361, 17103, 24627,  1058, 31246, 34487, 37158,\n",
       "          4512, 28258,  4896, 14530, 40952, 30313, 44251,  8572, 24940, 32069,\n",
       "         30549, 26022, 29078, 23132,  1371, 32959, 12180, 19029, 20681,  2525,\n",
       "         19586,  8131, 13268, 22703, 44018, 38133, 38902,  3138, 38040, 35338,\n",
       "         37240, 10381, 14720, 12819, 25579, 31849, 35224, 22791,  3500, 31618,\n",
       "          2686,   123,  9629, 22396, 30520, 31327, 21082,  2832, 19176,  6775,\n",
       "          6213, 13970, 27976,   652,   370, 30582, 13532,  9698,  6449,    75,\n",
       "         11580,  4441, 21010,  6487, 44174, 44503, 16329, 15344, 27644,   860],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'a person who is very knowledgeable about many subjects', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = type(model).__name__\n",
    "filename = f'../trained_models/{model_name} Epoch {epoch+1} at {datetime.datetime.now()}'.replace(' ', '_')\n",
    "with open(filename, 'wb+') as f:\n",
    "    torch.save({'state_dict': model.state_dict()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"../code/dataset.py\", line 254, in collate_fn\n    Yvecs = self.embeddings.get_vecs(Ys)\n  File \"../code/dataset.py\", line 55, in get_vecs\n    vecs = [self.embeddings[self.stoi[t]] for t in tokens]\n  File \"../code/dataset.py\", line 55, in <listcomp>\n    vecs = [self.embeddings[self.stoi[t]] for t in tokens]\nRuntimeError: CUDA error: initialization error\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b7dea689fb99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"../code/dataset.py\", line 254, in collate_fn\n    Yvecs = self.embeddings.get_vecs(Ys)\n  File \"../code/dataset.py\", line 55, in get_vecs\n    vecs = [self.embeddings[self.stoi[t]] for t in tokens]\n  File \"../code/dataset.py\", line 55, in <listcomp>\n    vecs = [self.embeddings[self.stoi[t]] for t in tokens]\nRuntimeError: CUDA error: initialization error\n"
     ]
    }
   ],
   "source": [
    "x, y, yvecs = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75100, 300])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "049faeb3947c48ac1b8702363c1a3bc597f6c2e1e1396be70d54511d980ab606"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
