{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "sys.path.append('../code')\n",
    "from dataset import get_data, MaskedDataset, make_vocab\n",
    "\n",
    "from transformers import (\n",
    "    AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from models import MaskedRDModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_size = 5\n",
    "# model = torch.load('../trained_models/MaskedRDModel_Epoch_1_at_2021-05-05_06:45:01.309818')\n",
    "# model = MaskedRDModel.from_pretrained('bert-base-uncased')\n",
    "# model.initialize(mask_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training data: 675715 word-def pairs\n",
      "Dev data: 75873 word-def pairs\n",
      "Test data: 1200 word-def pairs\n"
     ]
    }
   ],
   "source": [
    "d = get_data('../wantwords-english-baseline/data', word2vec=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_data_def, dev_data, test_data_seen, \\\n",
    "    test_data_unseen, test_data_desc = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_matrix, target2idx, idx2target = make_vocab(d, tokenizer, mask_size=mask_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16187, tensor([2338,  103,  103,  103,  103]), 'book')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target2idx maps target words to indices\n",
    "# target_matrix maps target indices to bpe sequences, padded/truncated to mask_size\n",
    "target2idx['book'], target_matrix[target2idx['book']], idx2target[target2idx['book']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MaskedDataset(train_data + train_data_def, tokenizer, target2idx, mask_size=mask_size)\n",
    "dev_dataset = MaskedDataset(dev_data, tokenizer, target2idx, mask_size=mask_size)\n",
    "test_dataset_seen = MaskedDataset(test_data_seen, tokenizer, target2idx, mask_size=mask_size)\n",
    "test_dataset_unseen = MaskedDataset(test_data_unseen, tokenizer, target2idx, mask_size=mask_size)\n",
    "test_dataset_desc = MaskedDataset(test_data_desc, tokenizer, target2idx, mask_size=mask_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 55\n",
    "num_workers = 4\n",
    "\n",
    "loader_params = {\n",
    "    'pin_memory': False,\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': num_workers,\n",
    "    'collate_fn': dev_dataset.collate_fn\n",
    "}\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, **{'shuffle': True, **loader_params})\n",
    "dev_loader = data.DataLoader(dev_dataset, **{'shuffle': True, **loader_params})\n",
    "test_loader_seen = data.DataLoader(test_dataset_seen, **{'shuffle': False, **loader_params})\n",
    "test_loader_unseen = data.DataLoader(test_dataset_unseen, **{'shuffle': False, **loader_params})\n",
    "test_loader_desc = data.DataLoader(test_dataset_desc, **{'shuffle': False, **loader_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([  101,   103,   103,   103,   103,   103,   102,  2000,  2713,  1037,\n",
       "           2711,  2013,  7750, 11819,  2013,  6531,  1037,  7979,  4735,  2040,\n",
       "           2001, 14933,  2098,  2011,  1996,  3099,   102]),\n",
       "  0),\n",
       " ['[CLS]',\n",
       "  '[MASK]',\n",
       "  '[MASK]',\n",
       "  '[MASK]',\n",
       "  '[MASK]',\n",
       "  '[MASK]',\n",
       "  '[SEP]',\n",
       "  'to',\n",
       "  'release',\n",
       "  'a',\n",
       "  'person',\n",
       "  'from',\n",
       "  'punishment',\n",
       "  'exempt',\n",
       "  'from',\n",
       "  'penalty',\n",
       "  'a',\n",
       "  'convicted',\n",
       "  'criminal',\n",
       "  'who',\n",
       "  'was',\n",
       "  'pardon',\n",
       "  '##ed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'governor',\n",
       "  '[SEP]'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0], tokenizer.convert_ids_to_tokens(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "lr = 2e-5\n",
    "optim = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "warmup_duration = 0.01 # portion of the first epoch spent on lr warmup\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=len(train_loader) * warmup_duration, \n",
    "                                            num_training_steps=len(train_loader) * epochs)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "# scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreverse-dict\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.29 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fancy-elevator-53</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/reverse-dict/reverse-dictionary\" target=\"_blank\">https://wandb.ai/reverse-dict/reverse-dictionary</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/reverse-dict/reverse-dictionary/runs/1a8nd46n\" target=\"_blank\">https://wandb.ai/reverse-dict/reverse-dictionary/runs/1a8nd46n</a><br/>\n",
       "                Run data is saved locally in <code>/home/ubuntu/dl/11785-Reverse-Dictionary-Project/notebooks/wandb/run-20210505_044914-1a8nd46n</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f0ea4382410>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project='reverse-dictionary', entity='reverse-dict')\n",
    "\n",
    "config = wandb.config\n",
    "config.learning_rate = lr\n",
    "config.epochs = epochs\n",
    "config.batch_size = batch_size\n",
    "config.optimizer = type(optim).__name__\n",
    "config.scheduler = type(scheduler).__name__\n",
    "config.warmup_duration = warmup_duration\n",
    "\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matrix = target_matrix.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred, gt, test=False):\n",
    "    acc1 = acc10 = acc100 = 0\n",
    "    n = len(pred)\n",
    "    pred_rank = []\n",
    "    for p, word in zip(pred, gt):\n",
    "        if test:\n",
    "            loc = (p == word).nonzero(as_tuple=True)\n",
    "            if len(loc) != 0:\n",
    "                pred_rank.append(min(loc[-1], 1000))\n",
    "            else:\n",
    "                pred_rank.append(1000)\n",
    "        if word in p[:100]:\n",
    "            acc100 += 1\n",
    "            if word in p[:10]:\n",
    "                acc10 += 1\n",
    "                if word == p[0]:\n",
    "                    acc1 += 1\n",
    "    if test:\n",
    "        pred_rank = torch.tensor(pred_rank, dtype=torch.float32)\n",
    "        return (acc1, acc10, acc100, pred_rank)\n",
    "    else:\n",
    "        return acc1/n, acc10/n, acc100/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07096cbef7b5442d8b009de77a667c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 142.00 MiB (GPU 0; 14.76 GiB total capacity; 12.69 GiB already allocated; 45.75 MiB free; 13.45 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-84e6aad5a7cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             loss, out = model(input_ids=x, attention_mask=attention_mask, \n\u001b[0;32m---> 22\u001b[0;31m                               target_matrix=target_matrix, ground_truth=y)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#             scaler.scale(loss).backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dl/11785-Reverse-Dictionary-Project/code/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, target_matrix, ground_truth, sep_id, wn_ids, weight_gt, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentence_embedding_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Simple MLP decoder --> modeled off of BERT MLM head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         self.decoder = nn.Sequential(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         )\n\u001b[1;32m    983\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 )\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         )\n\u001b[1;32m    463\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         )\n\u001b[1;32m    396\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 142.00 MiB (GPU 0; 14.76 GiB total capacity; 12.69 GiB already allocated; 45.75 MiB free; 13.45 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "inc = 10\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epoch, epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    # Train on subset of training data to save time\n",
    "    with tqdm(total=len(train_loader)) as pbar:\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            if i % inc == 0 and i != 0:\n",
    "                display_loss = train_loss / i\n",
    "                pbar.set_description(f'Epoch {epoch+1}, Train Loss: {train_loss / i}')\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            x = x.to(device)\n",
    "            attention_mask = (x != train_dataset.pad_id)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            loss, out = model(input_ids=x, attention_mask=attention_mask, \n",
    "                              target_matrix=target_matrix, ground_truth=y)\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "            loss.backward()\n",
    "            \n",
    "#             scaler.unscale_(optim)\n",
    "            nn.utils.clip_grad_value_(model.parameters(), 5)\n",
    "            \n",
    "#             scaler.step(optim)\n",
    "            optim.step()\n",
    "#             scaler.update()\n",
    "            \n",
    "            train_loss += loss.detach()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            del x, y, out, loss, attention_mask\n",
    "            \n",
    "    model_name = type(model).__name__\n",
    "    filename = f'../trained_models/{model_name} Epoch {epoch+1} at {datetime.datetime.now()}'.replace(' ', '_')\n",
    "    with open(filename, 'wb+') as f:\n",
    "        torch.save(model, f)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc1, val_acc10, val_acc100 = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(dev_loader)) as pbar:\n",
    "            for i, (x, y) in enumerate(dev_loader):\n",
    "                if i % inc == 0 and i != 0:\n",
    "                    display_loss = val_loss / i\n",
    "                    pbar.set_description(f'Epoch {epoch+1}, Val Loss: {val_loss / i}')\n",
    "\n",
    "                x = x.to(device)\n",
    "                attention_mask = (x != train_dataset.pad_id)\n",
    "                y = y.to(device)\n",
    "\n",
    "#                 with autocast():\n",
    "                loss, out = model(input_ids=x, attention_mask=attention_mask, target_matrix=target_matrix,\n",
    "                              criterion=criterion, ground_truth=y)\n",
    "\n",
    "                val_loss += loss.detach()\n",
    "\n",
    "                pbar.update(1)                \n",
    "                \n",
    "                result, indices = torch.topk(out, k=100, dim=-1, largest=True, sorted=True)\n",
    "                \n",
    "                acc1, acc10, acc100 = evaluate(indices, y)\n",
    "                val_acc1 += acc1\n",
    "                val_acc10 += acc10\n",
    "                val_acc100 += acc100\n",
    "\n",
    "                del x, y, out, loss\n",
    "    \n",
    "    wandb.log({\n",
    "        'train_loss': train_loss / len(train_loader),\n",
    "        'val_loss': val_loss / len(dev_loader),\n",
    "        'val_acc1': val_acc1 / len(dev_loader),\n",
    "        'val_acc10': val_acc10 / len(dev_loader),\n",
    "        'val_acc100': val_acc100 / len(dev_loader)\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Epoch 1, Train Loss: 5.3207106590271: 100%|██████████| 14078/14078 [1:39:36<00:00,  2.36it/s]   \n",
    "Epoch 1, Val Loss: 7.255414962768555: 100%|██████████| 1581/1581 [04:09<00:00,  6.34it/s] \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[SEP]',\n",
       " 'to',\n",
       " 'release',\n",
       " 'a',\n",
       " 'person',\n",
       " 'from',\n",
       " 'punishment',\n",
       " 'exempt',\n",
       " 'from',\n",
       " 'penalty',\n",
       " 'a',\n",
       " 'convicted',\n",
       " 'criminal',\n",
       " 'who',\n",
       " 'was',\n",
       " 'pardon',\n",
       " '##ed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'governor',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(train_dataset[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredFromDesc(model, desc : str, mask_size=5, top_n=10):\n",
    "    desc = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(desc))\n",
    "    cls_id, mask_id, sep_id, pad_id = dev_dataset.cls_id, dev_dataset.mask_id, dev_dataset.sep_id, dev_dataset.pad_id\n",
    "    desc_ids = [cls_id] + [mask_id] * mask_size + [sep_id] + desc + [sep_id]\n",
    "    x = torch.tensor(desc_ids).unsqueeze(0).to(device)\n",
    "    attention_mask = (x != pad_id)\n",
    "    out = model(input_ids=x, attention_mask=attention_mask, target_matrix=target_matrix)\n",
    "    result, indices = torch.topk(out, k=top_n, dim=-1, largest=True, sorted=True)\n",
    "    \n",
    "    indices = indices[0]\n",
    "    return [idx2target[i] for i in indices], indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['rustic',\n",
       "  'countryman',\n",
       "  'arctic',\n",
       "  'northerner',\n",
       "  'winters',\n",
       "  'outflank',\n",
       "  'deserter',\n",
       "  'landsman',\n",
       "  'country',\n",
       "  'landscapist',\n",
       "  'arcadian',\n",
       "  'sylvan',\n",
       "  'gypsy',\n",
       "  'highlander',\n",
       "  'quagmire',\n",
       "  'borer',\n",
       "  'northern',\n",
       "  'greengrocer',\n",
       "  'outlander',\n",
       "  'vegan',\n",
       "  'outlandish',\n",
       "  'alpine',\n",
       "  'spectrin',\n",
       "  'trappist',\n",
       "  'landside',\n",
       "  'chiller',\n",
       "  'southerner',\n",
       "  'quarantined',\n",
       "  'bushranger',\n",
       "  'barbaric',\n",
       "  'nordic',\n",
       "  'lurcher',\n",
       "  'barbarian',\n",
       "  'countrywoman',\n",
       "  'dendrite',\n",
       "  'desert',\n",
       "  'orientalist',\n",
       "  'inglenook',\n",
       "  'frozen',\n",
       "  'continental',\n",
       "  'inland',\n",
       "  'mountainous',\n",
       "  'borzoi',\n",
       "  'churlish',\n",
       "  'rustication',\n",
       "  'merganser',\n",
       "  'icepick',\n",
       "  'denizen',\n",
       "  'norther',\n",
       "  'cold',\n",
       "  'pastoralist',\n",
       "  'midland',\n",
       "  'pelagic',\n",
       "  'lutenist',\n",
       "  'bulgar',\n",
       "  'bouffant',\n",
       "  'winter',\n",
       "  'philistine',\n",
       "  'barranca',\n",
       "  'barman',\n",
       "  'outdoorsman',\n",
       "  'denier',\n",
       "  'bannister',\n",
       "  'snowman',\n",
       "  'polemicist',\n",
       "  'campervan',\n",
       "  'dendritic',\n",
       "  'lubricant',\n",
       "  'midwestern',\n",
       "  'icer',\n",
       "  'narcissist',\n",
       "  'borderland',\n",
       "  'cyclopean',\n",
       "  'bittern',\n",
       "  'unapologetic',\n",
       "  'furan',\n",
       "  'glacial',\n",
       "  'rural',\n",
       "  'deciduous',\n",
       "  'trouser',\n",
       "  'heathland',\n",
       "  'refugee',\n",
       "  'coldhearted',\n",
       "  'lugnut',\n",
       "  'bohemian',\n",
       "  'peregrine',\n",
       "  'meridional',\n",
       "  'barbeque',\n",
       "  'northland',\n",
       "  'siberian',\n",
       "  'landman',\n",
       "  'bushman',\n",
       "  'iceman',\n",
       "  'outlast',\n",
       "  'moorland',\n",
       "  'highland',\n",
       "  'lander',\n",
       "  'californian',\n",
       "  'proletarian',\n",
       "  'harsher'],\n",
       " tensor([16493, 26280, 16367, 27854, 14179, 22482, 15319, 40217, 21905,  2390,\n",
       "           169, 36353, 15094,  9948, 38077,  9426, 14713, 38814, 19714, 34149,\n",
       "         39356, 38866,  7512, 12622, 38036, 31285, 38760, 22095, 25413, 35246,\n",
       "         31187, 17946,  8324, 23123, 16423, 19627,   677, 15807, 33742, 21537,\n",
       "         10873,  1403,  8002, 23678,  8328, 10490, 48356, 21043, 34486, 13156,\n",
       "         34640, 17168, 17623, 31252, 36165, 16484, 26260, 40268, 17096, 11802,\n",
       "         46668, 18690, 11743,  9561, 38737, 39052, 44777, 19191, 20877, 48102,\n",
       "         46464, 25803, 36874, 12601,  2409, 39378, 35415, 27191, 33313,  1854,\n",
       "         32671, 24356,  8561,   638,  4860, 32369,  2527,  5990, 11185,  8383,\n",
       "         35051,  5777, 29936, 46255,  1435, 41112, 23026, 12137,  8090, 18225],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPredFromDesc(model, 'an inhabitant of a cold country', 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,   103,   103,   103,   103,   103,   102,  2583,  1998,  5627,\n",
       "          2000,  4553,  6570,  3085,  2402, 15608]),\n",
       " 75)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'teachable',\n",
       " 'lexnames': ['adj.all'],\n",
       " 'root_affix': ['able'],\n",
       " 'sememes': ['willing', 'undergo', 'teach'],\n",
       " 'definitions': 'able and willing to learn teachable youngsters'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [SEP] a whipping or flogging the discipline of the scourge'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(dev_dataset[120][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, name):\n",
    "    inc = 3\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc1 = test_acc10 = test_acc100 = test_rank_median = test_rank_variance = 0.0\n",
    "    total_seen = 0\n",
    "    all_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(loader)) as pbar:\n",
    "            for i, (x,y) in enumerate(loader):\n",
    "                if i % inc == 0 and i != 0:\n",
    "                    display_loss = test_loss / i\n",
    "                    pbar.set_description(f'Test Loss: {display_loss}')\n",
    "\n",
    "                x = x.to(device)\n",
    "                attention_mask = (x != dev_dataset.pad_id)\n",
    "                y = y.to(device)\n",
    "\n",
    "#                 with autocast():\n",
    "                loss, out = model(input_ids=x, attention_mask=attention_mask, \n",
    "                                  target_matrix=target_matrix, ground_truth=y)\n",
    "\n",
    "                test_loss += loss.detach()\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "                result, indices = torch.sort(out, descending=True)\n",
    "                \n",
    "                b = len(x)\n",
    "                acc1, acc10, acc100, pred_rank = evaluate(indices, y, test=True)\n",
    "                test_acc1 += acc1\n",
    "                test_acc10 += acc10\n",
    "                test_acc100 += acc100\n",
    "                total_seen += b\n",
    "                all_pred.extend(pred_rank)\n",
    "                \n",
    "                del x, y, out, loss\n",
    "                if i % 20 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "    test_loss /= len(loader)\n",
    "    test_acc1 /= total_seen\n",
    "    test_acc10 /= total_seen\n",
    "    test_acc100 /= total_seen\n",
    "    all_pred = torch.tensor(all_pred)\n",
    "    median = torch.median(all_pred)\n",
    "    var = torch.var(all_pred)**0.5\n",
    "    \n",
    "    print(f'{name}_test_loss:', test_loss)\n",
    "    print(f'{name}_test_acc1:', test_acc1)\n",
    "    print(f'{name}_test_acc10:', test_acc10)\n",
    "    print(f'{name}_test_acc100:', test_acc100)\n",
    "    print(f'{name}_test_rank_median:', median)\n",
    "    print(f'{name}_test_rank_variance', var)\n",
    "    \n",
    "    return ({\n",
    "        f'{name}_test_loss': test_loss,\n",
    "        f'{name}_test_acc1': test_acc1,\n",
    "        f'{name}_test_acc10': test_acc10,\n",
    "        f'{name}_test_acc100': test_acc100,\n",
    "        f'{name}_test_rank_median': test_rank_median,\n",
    "        f'{name}_test_rank_variance': test_rank_variance\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d871f3340b8f490cb734c8b4bd3c49cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_test_loss: tensor(5.4286, device='cuda:0')\n",
      "seen_test_acc1: 0.238\n",
      "seen_test_acc10: 0.456\n",
      "seen_test_acc100: 0.674\n",
      "seen_test_rank_median: tensor(13.)\n",
      "seen_test_rank_variance tensor(375.9349)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seen_test_loss': tensor(5.4286, device='cuda:0'),\n",
       " 'seen_test_acc1': 0.238,\n",
       " 'seen_test_acc10': 0.456,\n",
       " 'seen_test_acc100': 0.674,\n",
       " 'seen_test_rank_median': 0.0,\n",
       " 'seen_test_rank_variance': 0.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_seen, 'seen') # epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e495699ac3b24aa998ba7c6010004c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen_test_loss: tensor(7.1437, device='cuda:0')\n",
      "unseen_test_acc1: 0.114\n",
      "unseen_test_acc10: 0.302\n",
      "unseen_test_acc100: 0.5\n",
      "unseen_test_rank_median: tensor(99.)\n",
      "unseen_test_rank_variance tensor(435.7265)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unseen_test_loss': tensor(7.1437, device='cuda:0'),\n",
       " 'unseen_test_acc1': 0.114,\n",
       " 'unseen_test_acc10': 0.302,\n",
       " 'unseen_test_acc100': 0.5,\n",
       " 'unseen_test_rank_median': 0.0,\n",
       " 'unseen_test_rank_variance': 0.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_unseen, 'unseen') # epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bc3997ac424613a8e56a5664d7a44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_test_loss: tensor(2.8519, device='cuda:0')\n",
      "desc_test_acc1: 0.46\n",
      "desc_test_acc10: 0.82\n",
      "desc_test_acc100: 0.95\n",
      "desc_test_rank_median: tensor(1.)\n",
      "desc_test_rank_variance tensor(137.4574)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'desc_test_loss': tensor(2.8519, device='cuda:0'),\n",
       " 'desc_test_acc1': 0.46,\n",
       " 'desc_test_acc10': 0.82,\n",
       " 'desc_test_acc100': 0.95,\n",
       " 'desc_test_rank_median': 0.0,\n",
       " 'desc_test_rank_variance': 0.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_desc, 'desc') # epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22072ed64ac04c75b244f0bd4756c698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_test_loss: tensor(2.1239, device='cuda:0')\n",
      "seen_test_acc1: 0.61\n",
      "seen_test_acc10: 0.884\n",
      "seen_test_acc100: 0.926\n",
      "seen_test_rank_median: tensor(0.)\n",
      "seen_test_rank_variance tensor(227.7307)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seen_test_loss': tensor(2.1239, device='cuda:0'),\n",
       " 'seen_test_acc1': 0.61,\n",
       " 'seen_test_acc10': 0.884,\n",
       " 'seen_test_acc100': 0.926,\n",
       " 'seen_test_rank_median': 0.0,\n",
       " 'seen_test_rank_variance': 0.0}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_seen, 'seen') # epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f52ee489808490da6b9134e7cf28eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen_test_loss: tensor(9.5594, device='cuda:0')\n",
      "unseen_test_acc1: 0.078\n",
      "unseen_test_acc10: 0.308\n",
      "unseen_test_acc100: 0.528\n",
      "unseen_test_rank_median: tensor(68.)\n",
      "unseen_test_rank_variance tensor(434.0811)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unseen_test_loss': tensor(9.5594, device='cuda:0'),\n",
       " 'unseen_test_acc1': 0.078,\n",
       " 'unseen_test_acc10': 0.308,\n",
       " 'unseen_test_acc100': 0.528,\n",
       " 'unseen_test_rank_median': 0.0,\n",
       " 'unseen_test_rank_variance': 0.0}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_unseen, 'unseen') # epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b6d8b90ceb4b5fb0e99596c9ad66a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_test_loss: tensor(2.8336, device='cuda:0')\n",
      "desc_test_acc1: 0.42\n",
      "desc_test_acc10: 0.75\n",
      "desc_test_acc100: 0.935\n",
      "desc_test_rank_median: tensor(1.)\n",
      "desc_test_rank_variance tensor(97.9599)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'desc_test_loss': tensor(2.8336, device='cuda:0'),\n",
       " 'desc_test_acc1': 0.42,\n",
       " 'desc_test_acc10': 0.75,\n",
       " 'desc_test_acc100': 0.935,\n",
       " 'desc_test_rank_median': 0.0,\n",
       " 'desc_test_rank_variance': 0.0}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_desc, 'desc') # epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101, 103, 103,  ...,   0,   0,   0],\n",
       "        [101, 103, 103,  ...,   0,   0,   0],\n",
       "        [101, 103, 103,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [101, 103, 103,  ...,   0,   0,   0],\n",
       "        [101, 103, 103,  ...,   0,   0,   0],\n",
       "        [101, 103, 103,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, labels = next(iter(train_loader))\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_locations = torch.roll(input_ids == torch.tensor(102).expand_as(input_ids), shifts=1, dims=-1)\n",
    "sep_locations[:,0] = 0 # last [SEP] will wrap to 0th position\n",
    "token_type_ids = (torch.cumsum(sep_locations, dim=-1) > 0).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,   103,   103,   103,   103,   103,   102,  6331, 20976,  2819,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = type(model).__name__\n",
    "filename = f'../trained_models/{model_name} Epoch {epoch+1} at {datetime.datetime.now()}'.replace(' ', '_')\n",
    "with open(filename, 'wb+') as f:\n",
    "    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../trained_models/MaskedRDModel_Epoch_1_at_2021-05-05_06:45:01.309818'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code for printing with color, may use later\n",
    "class Color:\n",
    "    color = dict(\n",
    "        purple = '\\033[95m',\n",
    "        cyan = '\\033[96m',\n",
    "        darkCyan = '\\033[36m',\n",
    "        blue = '\\033[94m',\n",
    "        green = '\\033[92m',\n",
    "        yellow = '\\033[93m',\n",
    "        red = '\\033[91m'\n",
    "    )\n",
    "    bold = '\\033[1m'\n",
    "    underline = '\\033[4m'\n",
    "    end = '\\033[0m'\n",
    "    \n",
    "    \n",
    "def cprint(*args, **kwargs):\n",
    "    color = kwargs.pop('color', None)\n",
    "    color = Color.color.get(color, None)\n",
    "    bold = kwargs.pop('bold', False)\n",
    "    underline = kwargs.pop('underline', False)\n",
    "    end = kwargs.pop('end', '\\n')\n",
    "    if bold:\n",
    "        print(Color.bold, end='')\n",
    "    if underline:\n",
    "        print(Color.underline, end='')\n",
    "    if color is not None:\n",
    "        print(color, end='')    \n",
    "    print(*args, end='', **kwargs)\n",
    "    if color is not None:\n",
    "        print(Color.end, end='')\n",
    "    if bold:\n",
    "        print(Color.end, end='')\n",
    "    if underline:\n",
    "        print(Color.end, end='')\n",
    "    print(end=end)\n",
    "    \n",
    "def cstr(*args, color=None, bold=False, underline=False):\n",
    "    base = ' '.join(args)\n",
    "    display = []\n",
    "    color = Color.color.get(color, False)\n",
    "    if color:\n",
    "        display.append(color)\n",
    "    if underline:\n",
    "        display.append(Color.underline)\n",
    "    if bold:\n",
    "        display.append(Color.bold)\n",
    "    display.append(base)\n",
    "    display.extend([Color.end] * (underline + bold + bool(color)))\n",
    "    return ''.join(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mask_start = 1\n",
    "model.mask_size = mask_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../trained_models/MaskedRDModel_Epoch_1_at_2021-04-26_03:46:31.644159')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.xent_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c75ae50b954b19b0986b912e58d762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_test_loss: tensor(2.1239, device='cuda:0')\n",
      "seen_test_acc1: 0.61\n",
      "seen_test_acc10: 0.884\n",
      "seen_test_acc100: 0.926\n",
      "seen_test_rank_median: tensor(0.)\n",
      "seen_test_rank_variance tensor(227.7307)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c3a21552a6424b83bec97f7301988a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen_test_loss: tensor(9.5594, device='cuda:0')\n",
      "unseen_test_acc1: 0.078\n",
      "unseen_test_acc10: 0.308\n",
      "unseen_test_acc100: 0.528\n",
      "unseen_test_rank_median: tensor(68.)\n",
      "unseen_test_rank_variance tensor(434.0811)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dee25443b74d61b24b6e905d97433d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_test_loss: tensor(2.8336, device='cuda:0')\n",
      "desc_test_acc1: 0.42\n",
      "desc_test_acc10: 0.75\n",
      "desc_test_acc100: 0.935\n",
      "desc_test_rank_median: tensor(1.)\n",
      "desc_test_rank_variance tensor(97.9599)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'desc_test_loss': tensor(2.8336, device='cuda:0'),\n",
       " 'desc_test_acc1': 0.42,\n",
       " 'desc_test_acc10': 0.75,\n",
       " 'desc_test_acc100': 0.935,\n",
       " 'desc_test_rank_median': 0.0,\n",
       " 'desc_test_rank_variance': 0.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_seen, 'seen') # MaskedRDModel_Epoch_7_at_2021-04-26_14:13:53.041391\n",
    "test(test_loader_unseen, 'unseen')\n",
    "test(test_loader_desc, 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb973217a8e84ee4aa98dd50e709b4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_test_loss: tensor(4.4673, device='cuda:0')\n",
      "seen_test_acc1: 0.306\n",
      "seen_test_acc10: 0.568\n",
      "seen_test_acc100: 0.74\n",
      "seen_test_rank_median: tensor(5.)\n",
      "seen_test_rank_variance tensor(328.7286)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa309358dadc46d085d1b0706c144f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen_test_loss: tensor(7.3299, device='cuda:0')\n",
      "unseen_test_acc1: 0.092\n",
      "unseen_test_acc10: 0.266\n",
      "unseen_test_acc100: 0.494\n",
      "unseen_test_rank_median: tensor(108.)\n",
      "unseen_test_rank_variance tensor(432.9438)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da37334d835d48f0ab7a85204d6695d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_test_loss: tensor(2.9275, device='cuda:0')\n",
      "desc_test_acc1: 0.43\n",
      "desc_test_acc10: 0.81\n",
      "desc_test_acc100: 0.955\n",
      "desc_test_rank_median: tensor(1.)\n",
      "desc_test_rank_variance tensor(63.1195)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'desc_test_loss': tensor(2.9275, device='cuda:0'),\n",
       " 'desc_test_acc1': 0.43,\n",
       " 'desc_test_acc10': 0.81,\n",
       " 'desc_test_acc100': 0.955,\n",
       " 'desc_test_rank_median': 0.0,\n",
       " 'desc_test_rank_variance': 0.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_seen, 'seen') # \n",
    "test(test_loader_unseen, 'unseen')\n",
    "test(test_loader_desc, 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = {e['word'] for e in (train_data + train_data_def)}\n",
    "dev_words = {e['word'] for e in dev_data}\n",
    "test_unseen_words = {e['word'] for e in test_data_unseen}\n",
    "test_seen_words = {e['word'] for e in test_data_seen}\n",
    "test_desc_words = {e['word'] for e in test_data_desc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aunt',\n",
       " 'city',\n",
       " 'elephant',\n",
       " 'fight',\n",
       " 'forget',\n",
       " 'government',\n",
       " 'green',\n",
       " 'juice',\n",
       " 'prepare',\n",
       " 'prevent',\n",
       " 'strawberry',\n",
       " 'thanks',\n",
       " 'wood'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words.intersection(dev_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_seen_words.intersection(dev_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675715"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) + len(train_data_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75873"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = set()\n",
    "for w in [train_words, dev_words, test_unseen_words, test_seen_words, test_desc_words]:\n",
    "    total = {*total, *w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50477"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d58ea2f321b40e9a9e0091c275fa639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_test_loss: tensor(3.8748, device='cuda:0')\n",
      "seen_test_acc1: 0.242\n",
      "seen_test_acc10: 0.69\n",
      "seen_test_acc100: 0.882\n",
      "seen_test_rank_median: tensor(3.)\n",
      "seen_test_rank_variance tensor(246.8164)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0c11c41bfe4b9db22257cdc198c2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen_test_loss: tensor(6.0390, device='cuda:0')\n",
      "unseen_test_acc1: 0.114\n",
      "unseen_test_acc10: 0.366\n",
      "unseen_test_acc100: 0.656\n",
      "unseen_test_rank_median: tensor(23.)\n",
      "unseen_test_rank_variance tensor(382.3108)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52578c2e0ae7403988a1733a24058cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_test_loss: tensor(3.8807, device='cuda:0')\n",
      "desc_test_acc1: 0.275\n",
      "desc_test_acc10: 0.685\n",
      "desc_test_acc100: 0.89\n",
      "desc_test_rank_median: tensor(2.)\n",
      "desc_test_rank_variance tensor(152.8812)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'desc_test_loss': tensor(3.8807, device='cuda:0'),\n",
       " 'desc_test_acc1': 0.275,\n",
       " 'desc_test_acc10': 0.685,\n",
       " 'desc_test_acc100': 0.89,\n",
       " 'desc_test_rank_median': 0.0,\n",
       " 'desc_test_rank_variance': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader_seen, 'seen')\n",
    "test(test_loader_unseen, 'unseen')\n",
    "test(test_loader_desc, 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'a type of tree',\n",
    "    'the opposite of being happy',\n",
    "    'employee at a circus',\n",
    "    'a road on which cars can go quickly without stopping',\n",
    "    'a very intelligent person',\n",
    "    'a very smart person',\n",
    "    'something you use to measure your temperature',\n",
    "    'a dark time of day',\n",
    "    'medieval social hierarchy where peasants and vassals served lords',\n",
    "    'to help someone else learn',\n",
    "    'when someone you trust does something that breaks your trust',\n",
    "    'deep learning'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for a type of tree\n",
      "(['chestnut',\n",
      "  'spruce',\n",
      "  'pinewood',\n",
      "  'teakwood',\n",
      "  'linden',\n",
      "  'redwood',\n",
      "  'oak',\n",
      "  'maple',\n",
      "  'logwood',\n",
      "  'fir',\n",
      "  'hornbeam',\n",
      "  'mangrove',\n",
      "  'lime',\n",
      "  'cedarwood',\n",
      "  'hardwood',\n",
      "  'evergreen',\n",
      "  'mahogany',\n",
      "  'oakum',\n",
      "  'tree',\n",
      "  'applewood',\n",
      "  'pollard',\n",
      "  'birchbark',\n",
      "  'nopal',\n",
      "  'cypress',\n",
      "  'boxwood',\n",
      "  'plane',\n",
      "  'almond',\n",
      "  'limn',\n",
      "  'plum',\n",
      "  'loquat',\n",
      "  'pear',\n",
      "  'ashlar',\n",
      "  'aspen',\n",
      "  'fig',\n",
      "  'elderberry',\n",
      "  'dogwood',\n",
      "  'olive',\n",
      "  'poplar',\n",
      "  'hawthorn',\n",
      "  'barking',\n",
      "  'stocked',\n",
      "  'sandalwood',\n",
      "  'eucalyptus',\n",
      "  'firkin',\n",
      "  'rowan',\n",
      "  'pine',\n",
      "  'rosewood',\n",
      "  'treed',\n",
      "  'tea',\n",
      "  'limes',\n",
      "  'medlar',\n",
      "  'mango',\n",
      "  'chinquapin',\n",
      "  'oaken',\n",
      "  'plumb',\n",
      "  'nome',\n",
      "  'pineal',\n",
      "  'stocks',\n",
      "  'logarithm',\n",
      "  'hickory',\n",
      "  'satinwood',\n",
      "  'plumber',\n",
      "  'aliquot',\n",
      "  'balsa',\n",
      "  'ebony',\n",
      "  'oakleaf',\n",
      "  'cordwood',\n",
      "  'teat',\n",
      "  'logjam',\n",
      "  'birch',\n",
      "  'basil',\n",
      "  'cork',\n",
      "  'ssh',\n",
      "  'pinecone',\n",
      "  'hazelnut',\n",
      "  'nock',\n",
      "  'ssp',\n",
      "  'nutmeg',\n",
      "  'woodcut',\n",
      "  'log',\n",
      "  'beechwood',\n",
      "  'liming',\n",
      "  'treetop',\n",
      "  'sau',\n",
      "  'myrtle',\n",
      "  'brazilwood',\n",
      "  'sycamore',\n",
      "  'mulberry',\n",
      "  'pineapple',\n",
      "  'cedar',\n",
      "  'cottonwood',\n",
      "  'mast',\n",
      "  'hazel',\n",
      "  'blazed',\n",
      "  'elder',\n",
      "  'forest',\n",
      "  'coffee',\n",
      "  'locus',\n",
      "  'turkey',\n",
      "  'teak'],\n",
      " tensor([35239, 30887, 22248,  3398, 11683, 11847, 21233,  4171, 44267,  1662,\n",
      "        41227, 34668,  1751, 30973, 23065, 14757, 36541, 33700, 19190, 32233,\n",
      "        36534, 46083, 38270, 11498, 47100, 12583, 17135, 24964, 12925, 24402,\n",
      "        39679,  7672, 27755, 43825, 44975, 41880, 42932,  6516, 42985, 30198,\n",
      "         2918, 35214, 25893, 50431,  4823, 40429, 22808,  6880, 22703, 12889,\n",
      "        37161, 13544,  4301, 30638, 40848, 32991, 36838, 18537, 44041, 39430,\n",
      "        10638, 25778, 50325,  9950, 35378,  6659, 18392, 19460,  7851, 20870,\n",
      "         8322, 30126, 16506, 15222, 14889, 23989, 40181, 16892,  7150, 42145,\n",
      "        14218, 23317, 20979, 35932, 36080, 12553, 15302, 42372, 20465, 38054,\n",
      "         6686, 20496, 33988, 21243, 28483, 29880, 20899, 45568, 30615, 12255],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for the opposite of being happy\n",
      "(['sadness',\n",
      "  'discoloration',\n",
      "  'unhappy',\n",
      "  'imbecility',\n",
      "  'dissipation',\n",
      "  'misanthropy',\n",
      "  'unhappiness',\n",
      "  'inanity',\n",
      "  'happiness',\n",
      "  'idiosyncrasy',\n",
      "  'nondiscrimination',\n",
      "  'loneliness',\n",
      "  'emptiness',\n",
      "  'discontinuation',\n",
      "  'misbehavior',\n",
      "  'lunacy',\n",
      "  'misanthrope',\n",
      "  'sadomasochism',\n",
      "  'sadism',\n",
      "  'impulsivity',\n",
      "  'discontinuity',\n",
      "  'joyousness',\n",
      "  'dissipative',\n",
      "  'disappointment',\n",
      "  'dissimilarity',\n",
      "  'disconcert',\n",
      "  'melancholy',\n",
      "  'malevolence',\n",
      "  'inactivation',\n",
      "  'coldness',\n",
      "  'discolouration',\n",
      "  'depression',\n",
      "  'regret',\n",
      "  'idyllic',\n",
      "  'inactivity',\n",
      "  'aggravation',\n",
      "  'sorrow',\n",
      "  'abomination',\n",
      "  'infidelity',\n",
      "  'indolence',\n",
      "  'amenity',\n",
      "  'sorrowing',\n",
      "  'regretting',\n",
      "  'imbecilic',\n",
      "  'discourage',\n",
      "  'shamelessness',\n",
      "  'disco',\n",
      "  'discordance',\n",
      "  'sad',\n",
      "  'dethronement',\n",
      "  'misanthropic',\n",
      "  'selfishness',\n",
      "  'sleeplessness',\n",
      "  'deadness',\n",
      "  'nonproliferation',\n",
      "  'nonmilitary',\n",
      "  'nonviolent',\n",
      "  'cryopreservation',\n",
      "  'regretful',\n",
      "  'discomfiture',\n",
      "  'nothingness',\n",
      "  'sorrowful',\n",
      "  'vacuity',\n",
      "  'evilness',\n",
      "  'disconcerting',\n",
      "  'alienation',\n",
      "  'misery',\n",
      "  'coldhearted',\n",
      "  'happy',\n",
      "  'discontinuance',\n",
      "  'monogamy',\n",
      "  'suppleness',\n",
      "  'disconsolate',\n",
      "  'sadhana',\n",
      "  'dissident',\n",
      "  'desensitisation',\n",
      "  'nonaggressive',\n",
      "  'sorry',\n",
      "  'frivolity',\n",
      "  'aplenty',\n",
      "  'discolor',\n",
      "  'inevitability',\n",
      "  'folly',\n",
      "  'solitariness',\n",
      "  'dysfunction',\n",
      "  'barrenness',\n",
      "  'aphorism',\n",
      "  'sleepiness',\n",
      "  'cold',\n",
      "  'unbelief',\n",
      "  'sleepover',\n",
      "  'infatuation',\n",
      "  'tediousness',\n",
      "  'mad',\n",
      "  'immobilisation',\n",
      "  'boredom',\n",
      "  'peacefulness',\n",
      "  'condolences',\n",
      "  'joy',\n",
      "  'condolence'],\n",
      " tensor([  128, 29761, 11144, 50012,  7388,  3612, 31810, 15969, 29088, 41457,\n",
      "        11337, 42715, 25108,  2510, 32814, 32172, 23126, 10877, 37672, 32898,\n",
      "         9082, 17615, 13945, 34624, 49058,  3273, 36427, 17912, 32521, 17867,\n",
      "        10267, 11426, 22568,   589, 46262, 12854,   383, 20042,  3262, 38567,\n",
      "        11583, 14670, 12908,  7492,   688, 31706, 49750,  3889, 13147, 34384,\n",
      "         4491, 31882,  6850,  3087,  1342, 15042, 40315,  6511, 14043, 13257,\n",
      "        34022, 13200,  3811, 37669, 21268, 24867, 40591,  8561, 33673,  5052,\n",
      "         5816, 37457, 33076, 34701, 43145, 20449, 42116, 13213, 45388, 24570,\n",
      "        34106, 31437,  3090, 15794, 23211, 43533,  1459, 38925, 13156, 30068,\n",
      "         3198,  8886, 47125, 24595, 45006, 41563, 49118, 18923, 25831, 35316],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for employee at a circus\n",
      "(['circus',\n",
      "  'clown',\n",
      "  'clowning',\n",
      "  'coworker',\n",
      "  'trouper',\n",
      "  'beefeater',\n",
      "  'horseman',\n",
      "  'clownish',\n",
      "  'barker',\n",
      "  'busker',\n",
      "  'roustabout',\n",
      "  'stockbroker',\n",
      "  'cornhusker',\n",
      "  'ringer',\n",
      "  'butcher',\n",
      "  'knacker',\n",
      "  'greaser',\n",
      "  'stockman',\n",
      "  'concessionaire',\n",
      "  'butchers',\n",
      "  'knackered',\n",
      "  'crowder',\n",
      "  'showman',\n",
      "  'stampede',\n",
      "  'rollercoaster',\n",
      "  'guardsman',\n",
      "  'bullfighter',\n",
      "  'joker',\n",
      "  'magician',\n",
      "  'troupe',\n",
      "  'stockist',\n",
      "  'chandelier',\n",
      "  'buffalo',\n",
      "  'jockey',\n",
      "  'butchering',\n",
      "  'masquerader',\n",
      "  'ringleader',\n",
      "  'hangman',\n",
      "  'cager',\n",
      "  'puncher',\n",
      "  'valet',\n",
      "  'tinker',\n",
      "  'bulldozer',\n",
      "  'rider',\n",
      "  'tenter',\n",
      "  'stocker',\n",
      "  'skinner',\n",
      "  'danseuse',\n",
      "  'masher',\n",
      "  'pitman',\n",
      "  'monkey',\n",
      "  'manipulator',\n",
      "  'trekker',\n",
      "  'frier',\n",
      "  'geriatrician',\n",
      "  'masochist',\n",
      "  'gherkin',\n",
      "  'vaudevillian',\n",
      "  'bellman',\n",
      "  'panhandler',\n",
      "  'gypsy',\n",
      "  'cavalryman',\n",
      "  'roper',\n",
      "  'grenadier',\n",
      "  'journeyman',\n",
      "  'pituitary',\n",
      "  'flier',\n",
      "  'haberdasher',\n",
      "  'maser',\n",
      "  'butchery',\n",
      "  'crewman',\n",
      "  'vanquisher',\n",
      "  'conker',\n",
      "  'beef',\n",
      "  'jockeying',\n",
      "  'charioteer',\n",
      "  'groomer',\n",
      "  'hanger',\n",
      "  'baluster',\n",
      "  'ratter',\n",
      "  'impresario',\n",
      "  'performer',\n",
      "  'horsehair',\n",
      "  'mountebank',\n",
      "  'trombonist',\n",
      "  'grecian',\n",
      "  'horsehead',\n",
      "  'hackneyed',\n",
      "  'haulier',\n",
      "  'bailor',\n",
      "  'entertainer',\n",
      "  'handguard',\n",
      "  'carter',\n",
      "  'greasepaint',\n",
      "  'hessian',\n",
      "  'crozier',\n",
      "  'miller',\n",
      "  'quacker',\n",
      "  'cartier',\n",
      "  'counterterrorist'],\n",
      " tensor([27089, 40171, 37228, 23077, 30531, 33934, 31271,  8610, 22474, 15416,\n",
      "        29469, 29001, 11423, 33889, 31220, 27365, 42276, 30094, 32117, 22470,\n",
      "         9836, 40066, 46566, 29114, 35855, 12509, 31338, 38872, 35999, 24038,\n",
      "        40078, 23005, 12620, 17115, 12312, 34684, 31703,  8274, 44250,  3480,\n",
      "        10969, 25734, 41277, 33386, 18308,  6239,  2885, 14496, 40826, 16042,\n",
      "         5099, 20344,  3630,  6941, 14731,  3104,  6385, 22627, 33422,  1211,\n",
      "        15094,  4439, 33311, 17189, 24397, 23588, 48604, 12308, 23463, 49416,\n",
      "        11840, 30404,  6675, 35719,  4592, 29812, 28786,  9017, 15656,  1838,\n",
      "        21478,  2708, 47421,  7183, 13755, 13282, 40094,   634, 29784, 23509,\n",
      "        32693, 44326,  6002, 38004, 17155, 30764, 11485,  4415,   704,  9344],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for a road on which cars can go quickly without stopping\n",
      "(['drive',\n",
      "  'speedway',\n",
      "  'trackway',\n",
      "  'freeway',\n",
      "  'clip',\n",
      "  'street',\n",
      "  'tracked',\n",
      "  'cutaway',\n",
      "  'pike',\n",
      "  'tracks',\n",
      "  'straightway',\n",
      "  'slowing',\n",
      "  'thoroughfare',\n",
      "  'highway',\n",
      "  'turnpike',\n",
      "  'strip',\n",
      "  'flyway',\n",
      "  'taxiway',\n",
      "  'driveline',\n",
      "  'flashover',\n",
      "  'cutting',\n",
      "  'tracking',\n",
      "  'slipway',\n",
      "  'zip',\n",
      "  'roundabout',\n",
      "  'flash',\n",
      "  'swatter',\n",
      "  'straightaway',\n",
      "  'clipping',\n",
      "  'clearway',\n",
      "  'roadster',\n",
      "  'striping',\n",
      "  'riad',\n",
      "  'byway',\n",
      "  'flyaway',\n",
      "  'road',\n",
      "  'slick',\n",
      "  'drove',\n",
      "  'drivetrain',\n",
      "  'dragstrip',\n",
      "  'speed',\n",
      "  'pavement',\n",
      "  'swat',\n",
      "  'bumper',\n",
      "  'drag',\n",
      "  'express',\n",
      "  'swift',\n",
      "  'trail',\n",
      "  'lanes',\n",
      "  'quickstep',\n",
      "  'roads',\n",
      "  'track',\n",
      "  'thruway',\n",
      "  'ramp',\n",
      "  'chase',\n",
      "  'haste',\n",
      "  'beltway',\n",
      "  'alley',\n",
      "  'headway',\n",
      "  'laneway',\n",
      "  'hack',\n",
      "  'boulevard',\n",
      "  'cycleway',\n",
      "  'motorcade',\n",
      "  'parkway',\n",
      "  'rush',\n",
      "  'runway',\n",
      "  'race',\n",
      "  'cutout',\n",
      "  'quick',\n",
      "  'fast',\n",
      "  'roadside',\n",
      "  'swath',\n",
      "  'crossroad',\n",
      "  'pad',\n",
      "  'straight',\n",
      "  'flypast',\n",
      "  'accelerating',\n",
      "  'flyover',\n",
      "  'hurry',\n",
      "  'dash',\n",
      "  'coast',\n",
      "  'padding',\n",
      "  'rushing',\n",
      "  'slash',\n",
      "  'driver',\n",
      "  'ski',\n",
      "  'cutthroat',\n",
      "  'shuttle',\n",
      "  'busway',\n",
      "  'skive',\n",
      "  'fly',\n",
      "  'beat',\n",
      "  'trackside',\n",
      "  'taxi',\n",
      "  'driveway',\n",
      "  'cleared',\n",
      "  'fasting',\n",
      "  'passu',\n",
      "  'passed'],\n",
      " tensor([34151,  8640, 31165, 19354, 29298, 12457, 31980, 34844,  4526,   261,\n",
      "        49222, 39327, 29589, 14565, 13115, 38140,  7642, 11793, 26208, 11560,\n",
      "         6369, 30913, 45266, 38056, 20055, 39649,  8576,  2050, 16309,    64,\n",
      "        15250,  9195,  3873, 32800, 10736, 18268, 17171, 26193, 46498,   425,\n",
      "        48665, 34152, 21977,  1548, 22501, 19786, 32961, 32637, 38393, 13450,\n",
      "        25956, 34101, 34815, 36404, 19823, 20797, 19701, 43722, 17830,  7315,\n",
      "         5168, 42555,  9283, 34224, 35861, 24395, 28832,  1162, 43425, 16414,\n",
      "         4241, 21391, 41261, 32041, 17148, 20965, 32059,   494, 25999, 37618,\n",
      "        11887, 17536, 46878, 22518, 17942,  5615, 41662,  9105, 22355, 32750,\n",
      "         6408,  7038, 30799, 37829, 40953, 17262, 17264, 29769, 22695, 32656],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for a very intelligent person\n",
      "(['brainstorming',\n",
      "  'brains',\n",
      "  'brainstem',\n",
      "  'brain',\n",
      "  'brainchild',\n",
      "  'brainiac',\n",
      "  'brainpower',\n",
      "  'genius',\n",
      "  'braincase',\n",
      "  'fool',\n",
      "  'brilliant',\n",
      "  'fooling',\n",
      "  'idiot',\n",
      "  'brainy',\n",
      "  'brainwash',\n",
      "  'minder',\n",
      "  'intellectual',\n",
      "  'spark',\n",
      "  'sage',\n",
      "  'mind',\n",
      "  'sparking',\n",
      "  'brainwashing',\n",
      "  'smart',\n",
      "  'foolhardy',\n",
      "  'minding',\n",
      "  'bighead',\n",
      "  'flasher',\n",
      "  'intelligent',\n",
      "  'madcap',\n",
      "  'intellect',\n",
      "  'blab',\n",
      "  'crackpot',\n",
      "  'visionary',\n",
      "  'cracker',\n",
      "  'flash',\n",
      "  'smarting',\n",
      "  'philosopher',\n",
      "  'idiotic',\n",
      "  'brainless',\n",
      "  'bright',\n",
      "  'blabber',\n",
      "  'mindful',\n",
      "  'mad',\n",
      "  'crack',\n",
      "  'bigot',\n",
      "  'blather',\n",
      "  'blimp',\n",
      "  'brainwashed',\n",
      "  'godhead',\n",
      "  'sapient',\n",
      "  'mastermind',\n",
      "  'prophet',\n",
      "  'hothead',\n",
      "  'lunatic',\n",
      "  'scientist',\n",
      "  'sapper',\n",
      "  'crackling',\n",
      "  'wit',\n",
      "  'brave',\n",
      "  'saint',\n",
      "  'doctor',\n",
      "  'mindset',\n",
      "  'dragon',\n",
      "  'freak',\n",
      "  'smarts',\n",
      "  'cynic',\n",
      "  'clever',\n",
      "  'head',\n",
      "  'mordant',\n",
      "  'master',\n",
      "  'sparky',\n",
      "  'fox',\n",
      "  'rocketeer',\n",
      "  'rocket',\n",
      "  'deadhead',\n",
      "  'knowing',\n",
      "  'professor',\n",
      "  'technocrat',\n",
      "  'stupid',\n",
      "  'thinker',\n",
      "  'doctored',\n",
      "  'blatant',\n",
      "  'superman',\n",
      "  'headbutt',\n",
      "  'nerve',\n",
      "  'thunderhead',\n",
      "  'talker',\n",
      "  'sap',\n",
      "  'doctoring',\n",
      "  'snapper',\n",
      "  'teaser',\n",
      "  'dreamer',\n",
      "  'cypher',\n",
      "  'madder',\n",
      "  'bigamist',\n",
      "  'headman',\n",
      "  'sapsucker',\n",
      "  'politician',\n",
      "  'mindfulness',\n",
      "  'crank'],\n",
      " tensor([19970, 25579, 43943, 38338, 30253, 15336, 24301, 27923,  9668, 17206,\n",
      "         9645, 14092, 13296, 22053,  6669, 32666, 33895, 21549, 12819, 30287,\n",
      "        43732, 47941, 31720,  9685, 36344, 15433, 28137, 12896, 41984, 22791,\n",
      "        23967,  9922, 26837, 35759, 39649,  5311,  4896, 16647,  3983, 15624,\n",
      "         6090,  8616, 24595,  8092, 31772,   645,    79,   312, 49219, 29465,\n",
      "         3733, 22396, 27200, 25801,  4512,  4029, 39086, 24897, 14170, 10733,\n",
      "        30313, 37444, 28757, 41787, 33125,   482, 22441, 49345, 46277, 40203,\n",
      "        37441, 30484, 11042, 28611, 49041, 21410,  2972, 31246,  2607, 42798,\n",
      "        34487, 31218, 27379, 46523, 40368, 17309,  6231, 16670, 17576, 17033,\n",
      "        35516,  3357, 19374, 47127, 26551, 45618, 28833, 22132,  5316,  3853],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for a very smart person\n",
      "(['smart',\n",
      "  'flasher',\n",
      "  'brains',\n",
      "  'smarting',\n",
      "  'flash',\n",
      "  'smarts',\n",
      "  'brainstorming',\n",
      "  'brain',\n",
      "  'brainstem',\n",
      "  'brainchild',\n",
      "  'crackpot',\n",
      "  'spark',\n",
      "  'cracker',\n",
      "  'snapper',\n",
      "  'fool',\n",
      "  'hothead',\n",
      "  'brainpower',\n",
      "  'brilliant',\n",
      "  'smarty',\n",
      "  'crack',\n",
      "  'snap',\n",
      "  'sparking',\n",
      "  'genius',\n",
      "  'brainiac',\n",
      "  'fooling',\n",
      "  'idiot',\n",
      "  'hotdog',\n",
      "  'crackerjack',\n",
      "  'dogmatist',\n",
      "  'snappish',\n",
      "  'bomb',\n",
      "  'brave',\n",
      "  'flashy',\n",
      "  'arrowhead',\n",
      "  'jerker',\n",
      "  'bighead',\n",
      "  'flashover',\n",
      "  'dog',\n",
      "  'fox',\n",
      "  'brainy',\n",
      "  'blade',\n",
      "  'flashcard',\n",
      "  'hotshot',\n",
      "  'gop',\n",
      "  'hotpot',\n",
      "  'braincase',\n",
      "  'minder',\n",
      "  'bombast',\n",
      "  'snakehead',\n",
      "  'smartness',\n",
      "  'rocket',\n",
      "  'foolhardy',\n",
      "  'knack',\n",
      "  'hawk',\n",
      "  'sharp',\n",
      "  'jerk',\n",
      "  'nerve',\n",
      "  'dragon',\n",
      "  'punk',\n",
      "  'nip',\n",
      "  'madcap',\n",
      "  'brainwash',\n",
      "  'mad',\n",
      "  'sapsucker',\n",
      "  'sapper',\n",
      "  'tiger',\n",
      "  'smartly',\n",
      "  'gosling',\n",
      "  'bullet',\n",
      "  'blimp',\n",
      "  'grit',\n",
      "  'birder',\n",
      "  'crackling',\n",
      "  'bright',\n",
      "  'cocksucker',\n",
      "  'snappy',\n",
      "  'hotspot',\n",
      "  'tomato',\n",
      "  'mantrap',\n",
      "  'spearhead',\n",
      "  'crackles',\n",
      "  'teaser',\n",
      "  'cocker',\n",
      "  'mind',\n",
      "  'head',\n",
      "  'hotchpotch',\n",
      "  'sucker',\n",
      "  'horsehead',\n",
      "  'puncher',\n",
      "  'catnip',\n",
      "  'wit',\n",
      "  'chipper',\n",
      "  'rocketeer',\n",
      "  'blather',\n",
      "  'knacker',\n",
      "  'mover',\n",
      "  'hot',\n",
      "  'ticker',\n",
      "  'sparky',\n",
      "  'slicker'],\n",
      " tensor([31720, 28137, 25579,  5311, 39649, 33125, 19970, 38338, 43943, 30253,\n",
      "         9922, 21549, 35759, 17033, 17206, 27200, 24301,  9645, 30935,  8092,\n",
      "        20026, 43732, 27923, 15336, 14092, 13296,  9041, 13641, 25012, 29120,\n",
      "        43277, 14170, 33286, 45931, 17422, 15433, 11560,  4018, 30484, 22053,\n",
      "        40460, 23962, 25770, 27604, 11552,  9668, 32666, 35255,  1205, 38157,\n",
      "        28611,  9685, 41570, 33326, 22269, 31446, 40368, 28757, 28522,  1791,\n",
      "        41984,  6669, 24595, 28833,  4029, 25401, 19243, 34438, 39876,    79,\n",
      "        29499, 26050, 39086, 15624, 11029, 35023, 48296, 27795,   783, 35292,\n",
      "        11064, 35516, 43543, 30287, 49345,  6167, 30817, 40094,  3480, 43845,\n",
      "        24897, 15626, 11042,   645, 27365,  7146,  9961, 11576, 37441, 15702],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for something you use to measure your temperature\n",
      "(['thermometer',\n",
      "  'heater',\n",
      "  'calorimeter',\n",
      "  'temperature',\n",
      "  'heat',\n",
      "  'gasometer',\n",
      "  'barometer',\n",
      "  'heating',\n",
      "  'cooler',\n",
      "  'fahrenheit',\n",
      "  'conditioning',\n",
      "  'isotherm',\n",
      "  'micrometer',\n",
      "  'refrigerant',\n",
      "  'anemometer',\n",
      "  'refrigerate',\n",
      "  'pyrometer',\n",
      "  'refrigerating',\n",
      "  'radiometer',\n",
      "  'thermostat',\n",
      "  'rheometer',\n",
      "  'humidifier',\n",
      "  'bronchodilator',\n",
      "  'refrigeration',\n",
      "  'censer',\n",
      "  'tested',\n",
      "  'suntan',\n",
      "  'manometer',\n",
      "  'prefetch',\n",
      "  'cooling',\n",
      "  'inclinometer',\n",
      "  'ventilator',\n",
      "  'monitoring',\n",
      "  'timed',\n",
      "  'tachometer',\n",
      "  'magnetometer',\n",
      "  'incubator',\n",
      "  'spirometer',\n",
      "  'geometer',\n",
      "  'spacing',\n",
      "  'suntanned',\n",
      "  'oven',\n",
      "  'metering',\n",
      "  'climate',\n",
      "  'bathyscaphe',\n",
      "  'densitometer',\n",
      "  'meter',\n",
      "  'bathysphere',\n",
      "  'lather',\n",
      "  'thermal',\n",
      "  'homeostatic',\n",
      "  'calendar',\n",
      "  'tonometer',\n",
      "  'censure',\n",
      "  'photometer',\n",
      "  'cold',\n",
      "  'chiller',\n",
      "  'insulation',\n",
      "  'appreciator',\n",
      "  'potpourri',\n",
      "  'regulator',\n",
      "  'monitor',\n",
      "  'dosimeter',\n",
      "  'bencher',\n",
      "  'refrigerated',\n",
      "  'heated',\n",
      "  'groomer',\n",
      "  'furnace',\n",
      "  'fever',\n",
      "  'ventilation',\n",
      "  'potboiler',\n",
      "  'prelim',\n",
      "  'cooker',\n",
      "  'kelvin',\n",
      "  'cooke',\n",
      "  'feverfew',\n",
      "  'hotplate',\n",
      "  'falsifier',\n",
      "  'seismometer',\n",
      "  'bather',\n",
      "  'interferometer',\n",
      "  'thaler',\n",
      "  'stover',\n",
      "  'stove',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'refractor',\n",
      "  'toaster',\n",
      "  'measurer',\n",
      "  'bronchoscope',\n",
      "  'conditioner',\n",
      "  'measure',\n",
      "  'reflectometer',\n",
      "  'stovepiping',\n",
      "  'kisser',\n",
      "  'indicator',\n",
      "  'sundowner',\n",
      "  'certiorari',\n",
      "  'baths',\n",
      "  'prescriber',\n",
      "  'hydrometer',\n",
      "  'sun'],\n",
      " tensor([21694,  8653, 43736, 48277, 25497, 43191, 21219, 33404, 11401, 23558,\n",
      "        36418, 35931, 12915, 39099,  8840,  8385, 39573, 11440, 35187, 27127,\n",
      "        12871, 23728, 28154, 15343,  4497, 27122,  9253, 22133, 23868, 33803,\n",
      "        24097, 41980,  6213,  9326, 35758, 44024, 32228, 16616, 16406, 45168,\n",
      "        12573, 15141, 15495,  2795, 24506, 17738, 33029, 16321, 24800, 37815,\n",
      "        11812, 32769, 26743, 22997,  7510, 13156, 31285,  1979, 24940, 44376,\n",
      "        40195, 10077,  2775, 43405, 32063,  3168, 28786, 39450, 12720, 26158,\n",
      "        46524, 41527, 16928, 39157, 14266, 15161, 40759, 48130, 16710, 32286,\n",
      "         3187, 47240, 10929, 32534, 24713,   950, 39895, 28819, 40261,  4759,\n",
      "        49732, 11039,   301, 34296, 32030, 29867,   784, 10194, 22783,  8259],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for a dark time of day\n",
      "(['night',\n",
      "  'dusk',\n",
      "  'darkling',\n",
      "  'fark',\n",
      "  'darkness',\n",
      "  'darken',\n",
      "  'far',\n",
      "  'gloom',\n",
      "  'darky',\n",
      "  'shadowed',\n",
      "  'twilight',\n",
      "  'midnight',\n",
      "  'nightmarish',\n",
      "  'fart',\n",
      "  'evening',\n",
      "  'nightcap',\n",
      "  'farting',\n",
      "  'farthing',\n",
      "  'deep',\n",
      "  'dim',\n",
      "  'fard',\n",
      "  'darkie',\n",
      "  'faring',\n",
      "  'shadow',\n",
      "  'moon',\n",
      "  'nightie',\n",
      "  'nighthawk',\n",
      "  'shades',\n",
      "  'nightdress',\n",
      "  'darkish',\n",
      "  'shade',\n",
      "  'dusky',\n",
      "  'day',\n",
      "  'dimmer',\n",
      "  'nightfall',\n",
      "  'eve',\n",
      "  'nightlight',\n",
      "  'darkly',\n",
      "  'farad',\n",
      "  'moonshine',\n",
      "  'shady',\n",
      "  'nightlife',\n",
      "  'farce',\n",
      "  'faro',\n",
      "  'nightgown',\n",
      "  'farfalle',\n",
      "  'darkroom',\n",
      "  'moonlight',\n",
      "  'late',\n",
      "  'black',\n",
      "  'darkwave',\n",
      "  'dimming',\n",
      "  'shadowy',\n",
      "  'shadowing',\n",
      "  'mooning',\n",
      "  'dimmed',\n",
      "  'occult',\n",
      "  'farrow',\n",
      "  'close',\n",
      "  'nighttime',\n",
      "  'darkened',\n",
      "  'sinister',\n",
      "  'nightwear',\n",
      "  'blackout',\n",
      "  'gloomy',\n",
      "  'farro',\n",
      "  'low',\n",
      "  'nightwatchman',\n",
      "  'dead',\n",
      "  'smoke',\n",
      "  'lates',\n",
      "  'death',\n",
      "  'somber',\n",
      "  'quiet',\n",
      "  'morning',\n",
      "  'still',\n",
      "  'dak',\n",
      "  'lak',\n",
      "  'bourn',\n",
      "  'blacking',\n",
      "  'mor',\n",
      "  'farrowing',\n",
      "  'bore',\n",
      "  'moonlit',\n",
      "  'daybreak',\n",
      "  'loneliness',\n",
      "  'even',\n",
      "  'deepen',\n",
      "  'scowl',\n",
      "  'fade',\n",
      "  'farthest',\n",
      "  'nights',\n",
      "  'midsummer',\n",
      "  'long',\n",
      "  'farina',\n",
      "  'hades',\n",
      "  'sunset',\n",
      "  'raven',\n",
      "  'dau',\n",
      "  'lent'],\n",
      " tensor([30711, 23883, 17315, 38058, 10330, 42095, 49019, 36896, 28759, 44104,\n",
      "        18812,  5123, 25602, 40342, 32790, 44650,   883, 25854, 16815, 41664,\n",
      "        16684, 28658, 29366, 37961, 31928, 14942,  3778, 31799, 39101,  8159,\n",
      "        31443, 18941, 32315, 14032, 50201, 18649, 49233, 21620, 37055, 32625,\n",
      "        25002, 12426, 48622, 21577, 37541, 12754, 23636, 26205, 38339, 34961,\n",
      "         5372, 38843,  9632, 43350, 32199, 23961, 23895, 28421,  9786, 39568,\n",
      "         6094, 28495, 41067,  4257, 19554, 18578,  5073, 36583, 13574, 13751,\n",
      "        11978,  1975, 50204, 38599, 20406, 37903, 13791, 14625, 10518,  9855,\n",
      "        10325, 19431, 11925, 15303, 10764, 42715, 39805,  4245,  1112,  3165,\n",
      "        14057, 19595, 42896, 19378,  6037, 20942,  2201, 25930, 32386, 33335],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for medieval social hierarchy where peasants and vassals served lords\n",
      "(['dukedom',\n",
      "  'fiefdom',\n",
      "  'nobility',\n",
      "  'castle',\n",
      "  'polity',\n",
      "  'chivalry',\n",
      "  'hierarchy',\n",
      "  'vassalage',\n",
      "  'lords',\n",
      "  'gerontocracy',\n",
      "  'serval',\n",
      "  'magistracy',\n",
      "  'commune',\n",
      "  'vassal',\n",
      "  'feudalism',\n",
      "  'order',\n",
      "  'barony',\n",
      "  'feud',\n",
      "  'meritocracy',\n",
      "  'beneficiary',\n",
      "  'suzerainty',\n",
      "  'feudal',\n",
      "  'curacy',\n",
      "  'lordship',\n",
      "  'gentry',\n",
      "  'count',\n",
      "  'patrician',\n",
      "  'court',\n",
      "  'feuding',\n",
      "  'manor',\n",
      "  'barratry',\n",
      "  'gendarmerie',\n",
      "  'aristocracy',\n",
      "  'serfdom',\n",
      "  'castled',\n",
      "  'lord',\n",
      "  'obedience',\n",
      "  'counting',\n",
      "  'soc',\n",
      "  'polis',\n",
      "  'earldom',\n",
      "  'landlady',\n",
      "  'servitude',\n",
      "  'chastity',\n",
      "  'chieftaincy',\n",
      "  'courting',\n",
      "  'inquisition',\n",
      "  'countenance',\n",
      "  'ranked',\n",
      "  'paunchy',\n",
      "  'ranking',\n",
      "  'pol',\n",
      "  'nobleman',\n",
      "  'march',\n",
      "  'rusticity',\n",
      "  'caste',\n",
      "  'baron',\n",
      "  'ecclesia',\n",
      "  'laity',\n",
      "  'hustings',\n",
      "  'homage',\n",
      "  'crown',\n",
      "  'hussy',\n",
      "  'chapter',\n",
      "  'noble',\n",
      "  'justice',\n",
      "  'noblesse',\n",
      "  'beneficence',\n",
      "  'squire',\n",
      "  'archrival',\n",
      "  'knight',\n",
      "  'regality',\n",
      "  'county',\n",
      "  'manorial',\n",
      "  'army',\n",
      "  'clef',\n",
      "  'chivalric',\n",
      "  'rankle',\n",
      "  'society',\n",
      "  'comity',\n",
      "  'guild',\n",
      "  'civility',\n",
      "  'seriatim',\n",
      "  'familly',\n",
      "  'peasant',\n",
      "  'bourgeois',\n",
      "  'government',\n",
      "  'chiefdom',\n",
      "  'poling',\n",
      "  'landlord',\n",
      "  'theocracy',\n",
      "  'heterosis',\n",
      "  'chamberlain',\n",
      "  'ser',\n",
      "  'church',\n",
      "  'fealty',\n",
      "  'grange',\n",
      "  'raiding',\n",
      "  'rurality',\n",
      "  'service'],\n",
      " tensor([22299, 43377, 22184, 26177, 39652, 25317, 30647, 22063, 18960, 42055,\n",
      "        23479, 22228,  4431, 13253, 50177, 15160,  6389, 18471, 18843,  2505,\n",
      "        37846, 15563, 48579,  8560, 42087, 33044, 14437, 29739, 45596, 37213,\n",
      "        41553,  1332, 41717, 24657, 31482, 24469, 24102, 21047,  5205,  6749,\n",
      "        37800, 38688,  4777,  3211, 41610, 47249, 28790, 24559, 34716, 39650,\n",
      "        27530, 48192, 36969, 23839, 27731, 11967, 32815,  7417, 21925, 34208,\n",
      "        12360, 35812,  7792,  1035, 26478, 30963,  6714, 23959,  5262, 12758,\n",
      "        11496, 30910, 38102, 26269,  3282,  3378, 16581, 16354,   734, 16204,\n",
      "        39534, 30556, 35915,  7422,  8531, 36893, 44981,  4591, 42517, 27951,\n",
      "        40843, 15144,  1704, 36610, 44842,  1892, 35014, 42507, 40596, 39960],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for to help someone else learn\n",
      "(['help',\n",
      "  'assisted',\n",
      "  'helpdesk',\n",
      "  'second',\n",
      "  'aided',\n",
      "  'aid',\n",
      "  'helpmate',\n",
      "  'mentor',\n",
      "  'secondhand',\n",
      "  'mentored',\n",
      "  'helpmeet',\n",
      "  'answer',\n",
      "  'teach',\n",
      "  'follow',\n",
      "  'seconds',\n",
      "  'guided',\n",
      "  'supported',\n",
      "  'helper',\n",
      "  'supporting',\n",
      "  'helpline',\n",
      "  'consultancy',\n",
      "  'vet',\n",
      "  'represented',\n",
      "  'doctoring',\n",
      "  'advancing',\n",
      "  'doctor',\n",
      "  'tutoring',\n",
      "  'cover',\n",
      "  'disciple',\n",
      "  'tutor',\n",
      "  'support',\n",
      "  'communicate',\n",
      "  'helping',\n",
      "  'answering',\n",
      "  'doctored',\n",
      "  'relay',\n",
      "  'train',\n",
      "  'furthering',\n",
      "  'guide',\n",
      "  'further',\n",
      "  'contributing',\n",
      "  'rejoin',\n",
      "  'advance',\n",
      "  'learn',\n",
      "  'aiding',\n",
      "  'availing',\n",
      "  'handing',\n",
      "  'providing',\n",
      "  'compensate',\n",
      "  'represent',\n",
      "  'trained',\n",
      "  'ai',\n",
      "  'supply',\n",
      "  'avail',\n",
      "  'rewarding',\n",
      "  'advising',\n",
      "  'prompting',\n",
      "  'helpful',\n",
      "  'vetted',\n",
      "  'reward',\n",
      "  'assistant',\n",
      "  'exploited',\n",
      "  'assistance',\n",
      "  'bring',\n",
      "  'skilling',\n",
      "  'accompany',\n",
      "  'discipline',\n",
      "  'benefit',\n",
      "  'guiding',\n",
      "  'advise',\n",
      "  'skill',\n",
      "  'inform',\n",
      "  'accompanying',\n",
      "  'readied',\n",
      "  'attend',\n",
      "  'aide',\n",
      "  'handy',\n",
      "  'advantage',\n",
      "  'forwarding',\n",
      "  'cure',\n",
      "  'soothe',\n",
      "  'complement',\n",
      "  'encourage',\n",
      "  'tending',\n",
      "  'improve',\n",
      "  'defend',\n",
      "  'forward',\n",
      "  'managing',\n",
      "  'lesson',\n",
      "  'assistive',\n",
      "  'intervening',\n",
      "  'prove',\n",
      "  'feed',\n",
      "  'preparing',\n",
      "  'graduate',\n",
      "  'fetching',\n",
      "  'tend',\n",
      "  'provide',\n",
      "  'shepherd',\n",
      "  'knowledge'],\n",
      " tensor([10643, 32144,  5850,  5051,  1202, 10007, 10171, 37509, 38071, 43578,\n",
      "        23816, 38230, 39873, 24313,  2968, 18985, 26506,  5923, 18938, 23912,\n",
      "        27310, 13643, 38667, 17576, 33485, 30313,  7857, 29635, 42699, 20819,\n",
      "        42581,  7054, 26873, 14388, 34487, 14104, 18671, 34538, 23541, 48273,\n",
      "         2329, 23743,  2858, 33880, 43614, 38748,  7725, 44108,  4129,  5128,\n",
      "        30183, 20540, 36805, 39776, 24482, 29626, 37989, 17000, 37622,  6327,\n",
      "        11673, 19611, 23258,  8746, 32593, 42138,  1939,  7282, 39316, 43460,\n",
      "        11419, 11436, 42879,   555, 25982, 19157,  6026, 37447,  9016,  1565,\n",
      "        42318, 41671, 40976, 38262, 16486, 27352, 16663,   888,  2181, 40426,\n",
      "        34307, 17193, 38969, 19874, 17675, 35533, 14619, 32412, 34398,  8441],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for when someone you trust does something that breaks your trust\n",
      "(['trust',\n",
      "  'trusty',\n",
      "  'assuring',\n",
      "  'betray',\n",
      "  'betraying',\n",
      "  'judas',\n",
      "  'betrayal',\n",
      "  'blackmail',\n",
      "  'compromise',\n",
      "  'trustworthy',\n",
      "  'distrust',\n",
      "  'trustor',\n",
      "  'confidence',\n",
      "  'confraternity',\n",
      "  'friendship',\n",
      "  'compromised',\n",
      "  'trusting',\n",
      "  'security',\n",
      "  'fuck',\n",
      "  'bribe',\n",
      "  'trustful',\n",
      "  'letting',\n",
      "  'benevolence',\n",
      "  'trustworthiness',\n",
      "  'cheat',\n",
      "  'deception',\n",
      "  'confabulation',\n",
      "  'connivance',\n",
      "  'convoke',\n",
      "  'lie',\n",
      "  'break',\n",
      "  'conversing',\n",
      "  'mistake',\n",
      "  'confide',\n",
      "  'theft',\n",
      "  'cheated',\n",
      "  'adventuring',\n",
      "  'fuckup',\n",
      "  'shame',\n",
      "  'credits',\n",
      "  'threat',\n",
      "  'betrayer',\n",
      "  'faith',\n",
      "  'confiscation',\n",
      "  'fool',\n",
      "  'violated',\n",
      "  'lies',\n",
      "  'fooling',\n",
      "  'touch',\n",
      "  'menacing',\n",
      "  'revenge',\n",
      "  'insinuation',\n",
      "  'amiss',\n",
      "  'pawn',\n",
      "  'hostage',\n",
      "  'cliff',\n",
      "  'pawning',\n",
      "  'screw',\n",
      "  'breach',\n",
      "  'cliffhanger',\n",
      "  'breaching',\n",
      "  'fraud',\n",
      "  'caring',\n",
      "  'breakdancing',\n",
      "  'compromising',\n",
      "  'fragility',\n",
      "  'trustable',\n",
      "  'conceit',\n",
      "  'negligence',\n",
      "  'confidant',\n",
      "  'rob',\n",
      "  'conspiracy',\n",
      "  'breakbeat',\n",
      "  'frass',\n",
      "  'crook',\n",
      "  'confiding',\n",
      "  'danger',\n",
      "  'blackmailer',\n",
      "  'confidante',\n",
      "  'contrivance',\n",
      "  'gamble',\n",
      "  'trusted',\n",
      "  'sacrifice',\n",
      "  'love',\n",
      "  'config',\n",
      "  'embarrassment',\n",
      "  'protection',\n",
      "  'abuse',\n",
      "  'bond',\n",
      "  'compliance',\n",
      "  'crack',\n",
      "  'fraudulence',\n",
      "  'friend',\n",
      "  'marriage',\n",
      "  'involvement',\n",
      "  'harm',\n",
      "  'venture',\n",
      "  'compromiser',\n",
      "  'alluring',\n",
      "  'fortissimo'],\n",
      " tensor([20311,  2512, 27615, 17674, 49268, 30372, 46568, 21692, 30919, 18136,\n",
      "        38624, 10161, 26847, 14513, 38734, 28644,  5133, 44038, 33253, 28370,\n",
      "        34524, 36880,  3159, 34497, 13001, 25891, 10290, 27049, 49587, 44409,\n",
      "         1633, 25769,  6007,  8244,  1724, 34030, 15509, 43630,  1612, 28446,\n",
      "         3089, 23663, 38200,  4634, 17206, 18794, 31348, 14092,  7490, 34733,\n",
      "        25903, 44002, 34773, 36288, 16306,  6293, 44747, 40459, 43175, 11726,\n",
      "        10837,  4721, 25025, 33878, 43112, 26811, 38057, 41540, 14458, 31821,\n",
      "        15682, 35601, 11665, 12130, 43563, 26571, 27656, 42160, 44417, 14752,\n",
      "        15901, 47528, 42722, 11183, 34197, 30467,  8478, 35280,  6982, 32620,\n",
      "         8092, 48912, 29828, 18774, 10220, 28008, 36813, 15457, 46191,  5944],\n",
      "       device='cuda:0'))\n",
      "\n",
      "Results for deep learning\n",
      "(['study',\n",
      "  'lore',\n",
      "  'science',\n",
      "  'research',\n",
      "  'education',\n",
      "  'scholarship',\n",
      "  'learning',\n",
      "  'enlightenment',\n",
      "  'academy',\n",
      "  'knowledge',\n",
      "  'culture',\n",
      "  'seminary',\n",
      "  'philosophy',\n",
      "  'conservatory',\n",
      "  'inquiry',\n",
      "  'instruction',\n",
      "  'introspection',\n",
      "  'analysis',\n",
      "  'profound',\n",
      "  'scientism',\n",
      "  'researching',\n",
      "  'literature',\n",
      "  'profanity',\n",
      "  'seminar',\n",
      "  'experience',\n",
      "  'deep',\n",
      "  'lesson',\n",
      "  'scission',\n",
      "  'cultured',\n",
      "  'training',\n",
      "  'reading',\n",
      "  'solipsism',\n",
      "  'profanation',\n",
      "  'tuition',\n",
      "  'university',\n",
      "  'studious',\n",
      "  'meditation',\n",
      "  'discovery',\n",
      "  'schoolwork',\n",
      "  'studied',\n",
      "  'civilization',\n",
      "  'examination',\n",
      "  'exploration',\n",
      "  'mastery',\n",
      "  'school',\n",
      "  'didacticism',\n",
      "  'disciplined',\n",
      "  'scholar',\n",
      "  'appreciation',\n",
      "  'profundity',\n",
      "  'theory',\n",
      "  'erudition',\n",
      "  'sci',\n",
      "  'invigilation',\n",
      "  'history',\n",
      "  'refining',\n",
      "  'traduction',\n",
      "  'introversion',\n",
      "  'tradition',\n",
      "  'synthesis',\n",
      "  'technique',\n",
      "  'disciple',\n",
      "  'revelation',\n",
      "  'recapitulation',\n",
      "  'cultivation',\n",
      "  'catechumen',\n",
      "  'studentship',\n",
      "  'indoctrination',\n",
      "  'profoundness',\n",
      "  'intellection',\n",
      "  'exposition',\n",
      "  'mastering',\n",
      "  'prof',\n",
      "  'learn',\n",
      "  'thought',\n",
      "  'recollection',\n",
      "  'acumen',\n",
      "  'professing',\n",
      "  'studio',\n",
      "  'wisdom',\n",
      "  'recitation',\n",
      "  'mysticism',\n",
      "  'penetration',\n",
      "  'divination',\n",
      "  'art',\n",
      "  'musicology',\n",
      "  'experimenting',\n",
      "  'dissertation',\n",
      "  'studying',\n",
      "  'specialization',\n",
      "  'pursuit',\n",
      "  'profiling',\n",
      "  'elaborated',\n",
      "  'involution',\n",
      "  'deepening',\n",
      "  'academicism',\n",
      "  'sophistication',\n",
      "  'botany',\n",
      "  'questing',\n",
      "  'graduation'],\n",
      " tensor([ 3500,  3138, 32959, 40975, 36247, 35224, 29078, 14644, 25031,  8441,\n",
      "        24577,  7595, 41042,  2901, 44266, 30971,  3956, 32210, 17267,  7748,\n",
      "        18693, 10273,  3371, 11034,  8832, 16815,  2181, 38326, 18859, 12186,\n",
      "        41191,  3390, 40068,  5298, 37240,  9175, 24988, 27884, 25902, 30520,\n",
      "        12649, 41461, 38330, 32709, 17639, 16819, 19925,  1930, 32820, 13896,\n",
      "        36984, 14530, 23377, 23381, 35848, 25395, 43166, 23952, 34332, 40956,\n",
      "         9518, 42699,  2786, 25928, 23842, 33904, 19998, 41064, 15622, 22628,\n",
      "        44449, 28376, 27145, 33880, 25392, 26939, 30142,  2114, 31925,    39,\n",
      "        37905, 44464, 19092, 38404,  8586,  7112,  4035,  2464, 46659,  7384,\n",
      "        22362, 15516, 28822, 26843, 24116,  1016, 26306, 14270,  9237, 13797],\n",
      "       device='cuda:0'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for q in queries:\n",
    "    print(f'Results for {q}')\n",
    "    pprint(getPredFromDesc(model, q, top_n=100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "049faeb3947c48ac1b8702363c1a3bc597f6c2e1e1396be70d54511d980ab606"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
